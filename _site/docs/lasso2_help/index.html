<!doctype html>
<!--
  Minimal Mistakes Jekyll Theme 4.11.1 by Michael Rose
  Copyright 2013-2018 Michael Rose - mademistakes.com | @mmistakes
  Free for personal and commercial use under the MIT license
  https://github.com/mmistakes/minimal-mistakes/blob/master/LICENSE.txt
-->
<html lang="en" class="no-js">
  <head>
    <meta charset="utf-8">

<!-- begin _includes/seo.html --><title>Help file: lasso2 - The Stata Lasso Page</title>
<meta name="description" content="The Stata Lasso Page.">



<meta property="og:type" content="website">
<meta property="og:locale" content="en_US">
<meta property="og:site_name" content="The Stata Lasso Page">
<meta property="og:title" content="Help file: lasso2">
<meta property="og:url" content="http://localhost:4000/docs/lasso2_help/">












  

  


<link rel="canonical" href="http://localhost:4000/docs/lasso2_help/">







  <script type="application/ld+json">
    {
      "@context": "http://schema.org",
      "@type": "Person",
      "name": "Achim Ahrens",
      "url": "http://localhost:4000",
      "sameAs": null
    }
  </script>







<!-- end _includes/seo.html -->


<link href="/feed.xml" type="application/atom+xml" rel="alternate" title="The Stata Lasso Page Feed">

<!-- http://t.co/dKP3o1e -->
<meta name="HandheldFriendly" content="True">
<meta name="MobileOptimized" content="320">
<meta name="viewport" content="width=device-width, initial-scale=1.0">

<script>
  document.documentElement.className = document.documentElement.className.replace(/\bno-js\b/g, '') + ' js ';
</script>

<!-- For all browsers -->
<link rel="stylesheet" href="/assets/css/main.css">

<!--[if lte IE 9]>
  <style>
    /* old IE unsupported flexbox fixes */
    .greedy-nav .site-title {
      padding-right: 3em;
    }
    .greedy-nav button {
      position: absolute;
      top: 0;
      right: 0;
      height: 100%;
    }
  </style>
<![endif]-->


    <!-- start custom head snippets -->

<!-- insert favicons. use https://realfavicongenerator.net/ -->

<!-- end custom head snippets -->

  </head>

  <body class="layout--single wide">

    <!--[if lt IE 9]>
<div class="notice--danger align-center" style="margin: 0;">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience.</div>
<![endif]-->

    <div class="masthead">
  <div class="masthead__inner-wrap">
    <div class="masthead__menu">
      <nav id="site-nav" class="greedy-nav">
        <a class="site-title" href="/">The Stata Lasso Page</a>
        <ul class="visible-links">
          
            
            <li class="masthead__menu-item">
              <a href="http://localhost:4000/" >Home</a>
            </li>
          
            
            <li class="masthead__menu-item">
              <a href="http://localhost:4000/installation/" >Installation</a>
            </li>
          
            
            <li class="masthead__menu-item">
              <a href="http://localhost:4000/slides/" >Slides</a>
            </li>
          
            
            <li class="masthead__menu-item">
              <a href="http://localhost:4000/about/" >About</a>
            </li>
          
            
            <li class="masthead__menu-item">
              <a href="http://localhost:4000/references/" >References</a>
            </li>
          
        </ul>
        
        <button class="greedy-nav__toggle hidden" type="button">
          <span class="visually-hidden">Toggle Menu</span>
          <div class="navicon"></div>
        </button>
        <ul class="hidden-links hidden"></ul>
      </nav>
    </div>
  </div>
</div>

    <div class="initial-content">
      



<div id="main" role="main">
  
  <div class="sidebar sticky">
  
  
    
      
      
      
    
    
      

<nav class="nav__list">
  
  <input id="ac-toc" name="accordion-toc" type="checkbox" />
  <label for="ac-toc">Toggle Menu</label>
  <ul class="nav__items">
    
      <li>
        
          <span class="nav__sub-title">lassopack</span>
        

        
        <ul>
          
            
            

            
            

            <li><a href="/docs/lassopack/" class="">Overview</a></li>
          
            
            

            
            

            <li><a href="/docs/regularized_reg/" class="">Regularized regression</a></li>
          
            
            

            
            

            <li><a href="/docs/estimators/" class="">Estimation methods</a></li>
          
            
            

            
            

            <li><a href="/docs/lasso2/" class="">Getting started (lasso2)</a></li>
          
            
            

            
            

            <li><a href="/docs/cvlasso/" class="">Cross-validation (cvlasso)</a></li>
          
            
            

            
            

            <li><a href="/docs/rlasso/" class="">Rigorous lasso (rlasso)</a></li>
          
            
            

            
            

            <li><a href="/docs/lasso2_help/" class="active">Help lasso2</a></li>
          
            
            

            
            

            <li><a href="/docs/cvlasso_help/" class="">Help cvlasso</a></li>
          
            
            

            
            

            <li><a href="/docs/rlasso_help/" class="">Help rlasso</a></li>
          
            
            

            
            

            <li><a href="/docs/lassopack_cite/" class="">Citation</a></li>
          
        </ul>
        
      </li>
    
      <li>
        
          <span class="nav__sub-title">pdslasso</span>
        

        
        <ul>
          
            
            

            
            

            <li><a href="/docs/pdslasso/" class="">Overview</a></li>
          
            
            

            
            

            <li><a href="/docs/pdslasso_models/" class="">Models</a></li>
          
            
            

            
            

            <li><a href="/docs/pdslasso_demo/" class="">Demonstration</a></li>
          
            
            

            
            

            <li><a href="/docs/ivlasso_help/" class="">Help file</a></li>
          
            
            

            
            

            <li><a href="/docs/pdslasso_cite/" class="">Citation</a></li>
          
        </ul>
        
      </li>
    
  </ul>
</nav>
    
  
  </div>


  <article class="page" itemscope itemtype="http://schema.org/CreativeWork">
    <meta itemprop="headline" content="Help file: lasso2">
    
    
    

    <div class="page__inner-wrap">
      
        <header>
          <h1 id="page-title" class="page__title" itemprop="headline">Help file: lasso2
</h1>
          
        </header>
      

      <section class="page__content" itemprop="text">
        
        <pre id="stlog-1" style="font-size: 12px" class="sthlp"><b>help lasso2</b>
-------------------------------------------------------------------------------------------------------------------

<b><u>Title</u></b>

    <b>lasso2</b> --  Program for lasso, square-root lasso, elastic net, ridge, adaptive lasso and post-estimation OLS

<a name="stlog-1-syntax"></a><b><u>Syntax</u></b>

    Full syntax

        <b>lasso2</b> <i>depvar</i> <i>regressors</i> [<b>if</b> <i>exp</i>] [<b>in</b> <i>range</i>] [<b>,</b> <b><u>a</u></b><b>lpha(</b><i>real</i><b>)</b> <b>sqrt</b> <b><u>ada</u></b><b>ptive</b> <b><u>adal</u></b><b>oadings(</b><i>string</i><b>)</b>
              <b><u>adat</u></b><b>heta(</b><i>real</i><b>)</b> <b>ols</b> <b><u>l</u></b><b>ambda(</b><i>real</i><b>)</b> <b><u>lc</u></b><b>ount(</b><i>integer</i><b>)</b> <b><u>lminr</u></b><b>atio(</b><i>real</i><b>)</b> <b><u>lmax</u></b><b>(</b><i>real</i><b>)</b> <b><u>notp</u></b><b>en(</b><i>varlist</i><b>)</b>
              <b><u>par</u></b><b>tial(</b><i>varlist</i><b>)</b> <b><u>nor</u></b><b>ecover</b> <b><u>pload</u></b><b>ings(</b><i>string</i><b>)</b> <b><u>unitl</u></b><b>oadings</b> <b>prestd</b> <b><u>stdc</u></b><b>oef</b> <b>fe</b> <b>noftools</b> <b><u>noc</u></b><b>onstant</b>
              <b><u>tolo</u></b><b>pt(</b><i>real</i><b>)</b> <b><u>tolz</u></b><b>ero(</b><i>real</i><b>)</b> <b><u>maxi</u></b><b>ter(</b><i>int</i><b>)</b> <b><u>plot</u></b><b>path(</b><i>method</i><b>)</b> <b><u>plotv</u></b><b>ar(</b><i>varlist</i><b>)</b> <b><u>ploto</u></b><b>pt(</b><i>string</i><b>)</b> <b><u>plotl</u></b><b>abel</b>
              <b>ic(</b><i>string</i><b>)</b> <b>lic(</b><i>string</i><b>)</b> <b><u>ebicg</u></b><b>amma(</b><i>real</i><b>)</b> <b>noic</b> <b>long</b> <b>displayall</b> <b>postall</b> <b>postest</b> <b><u>ver</u></b><b>bose</b> <b><u>vver</u></b><b>bose</b> <b>wnorm</b>]

        Note: the <b>fe</b> option will take advantage of the <a href="#stlog-1-SG2016"><b>ftools</b></a> package (if installed) for the fixed-effects
              transformation; the speed gains using this package can be large.  See help ftools or click on ssc
              install ftools to install.

    <i>Estimators</i>            Description
    -------------------------------------------------------------------------------------------------------------
    <b><u>a</u></b><b>lpha(</b><i>real</i><b>)</b>            elastic net parameter, which controls the degree of L1-norm (lasso-type) to L2-norm
                            (ridge-type) penalization.  alpha=1 corresponds to the lasso (the default estimator),
                            and alpha=0 to ridge regression.  alpha must be in the interval [0,1].
    <b>sqrt</b>                   square-root lasso estimator.
    <b><u>ada</u></b><b>ptive</b>               adaptive lasso estimator.  The penalty loading for predictor j is set to
                            1/abs(beta0(j))^theta where beta0(j) is the OLS estimate or univariate OLS estimate
                            if p&gt;n.  Theta is the adaptive exponent, and can be controlled using the
                            <b><u>adat</u></b><b>heta(</b><i>real</i><b>)</b> option.
    <b><u>adal</u></b><b>oadings(</b><i>string</i><b>)</b>    alternative initial estimates, beta0, used for calculating adaptive loadings.  For
                            example, this could be the vector e(b) from an initial <b>lasso2</b> estimation.  The
                            elements of the vector are raised to the power -theta (note the minus).  See <b><u>ada</u></b><b>ptive</b>
                            option.
    <b><u>adat</u></b><b>heta(</b><i>real</i><b>)</b>         exponent for calculating adaptive penalty loadings. See <b><u>ada</u></b><b>ptive</b> option. Default=1.
    <b>ols</b>                    post-estimation OLS.  If lambda is a list, post-estimation OLS results are displayed
                            and returned in <b>e(betas)</b>.  If lambda is a scalar, post-estimation OLS is always
                            displayed, and this option controls whether standard or post-estimation OLS results
                            are stored in <b>e(b)</b>.
    -------------------------------------------------------------------------------------------------------------
    See overview of <a href="#stlog-1-estimators">estimation methods</a>.

    <i>Lambda(s)</i>             Description
    -------------------------------------------------------------------------------------------------------------
    <b><u>l</u></b><b>ambda(</b><i>numlist</i><b>)</b>        a scalar lambda value or list of descending lambda values. Each lambda value must be
                            greater than 0.  If not specified, the default list is used which is given by
                            <b>exp(rangen(log(lmax),log(lminratio*lmax),lcount))</b> (see <a href="http://www.stata.com/help.cgi?mf_range"><b>mf_range</b></a>).
    <b><u>lc</u></b><b>ount(</b><i>integer</i><b>)</b>†       number of lambda values for which the solution is obtained. Default is 100.
    <b><u>lminr</u></b><b>atio(</b><i>real</i><b>)</b>†       ratio of minimum to maximum lambda. <b>lminratio</b> must be between 0 and 1. Default is
                            1/1000.
    <b><u>lmax</u></b><b>(</b><i>real</i><b>)</b>†            maximum lambda value. Default is 2*max(X'y), and max(X'y) in the case of the
                            square-root lasso (where X is the pre-standardized regressor matrix and y is the
                            vector of the response variable).
    <b>lic(</b><i>string</i><b>)</b>            after first <b>lasso2</b> estimation using list of lambdas, estimate model corresponding to
                            minimum information criterion.  'aic', 'bic', 'aicc', and 'ebic' (the default) are
                            allowed.  Note the lower case spelling.  See <a href="#stlog-1-informationcriteria">Information criteria</a> for the definition
                            of each information criterion.
    <b><u>ebicg</u></b><b>amma(</b><i>real</i><b>)</b>        controls the <i>gamma</i> parameter of the EBIC.  <i>gamma</i> needs to lie in the [0,1] interval.
                            <i>gamma</i>=0 is equivalent to the BIC.  The default choice is <i>gamma</i>=1-log(n)/(2*log(p)).
    <b>postest</b>                Used in combination with <b>lic()</b>.  Stores estimation results of the model selected by
                            information criterion in <b>e()</b>.
    -------------------------------------------------------------------------------------------------------------
    † Not applicable if lambda() is specified.

    <i>Loadings &amp; standardization</i> Description
    -------------------------------------------------------------------------------------------------------------
    <b><u>notp</u></b><b>en(</b><i>varlist</i><b>)</b>        sets penalty loadings to zero for predictors in <i>varlist</i>.  Unpenalized predictors are
                            always included in the model.
    <b><u>par</u></b><b>tial(</b><i>varlist</i><b>)</b>       variables in <i>varlist</i> are partialled out prior to estimation.
    <b><u>nor</u></b><b>ecover</b>              suppresses recovery of partialled out variables after estimation.
    <b><u>pload</u></b><b>ings(</b><i>matrix</i><b>)</b>      a row-vector of penalty loadings; overrides the default standardization loadings (in
                            the case of the lasso, =sqrt(avg(x^2))).  The size of the vector should equal the
                            number of predictors (excluding partialled out variables and excluding the constant).
    <b><u>unitl</u></b><b>oadings</b>           penalty loadings set to a vector of ones; overrides the default standardization
                            loadings (in the case of the lasso, =sqrt(avg(x^2)).
    <b><u>pres</u></b><b>td</b>                 dependent variable and predictors are standardized prior to estimation rather than
                            standardized "on the fly" using penalty loadings.  See <a href="#stlog-1-standardization">here</a> for more details.  By
                            default the coefficient estimates are un-standardized (i.e., returned in original
                            units).
    <b><u>stdc</u></b><b>oef</b>                return coefficients in standard deviation units, i.e., don't un-standardize.  Only
                            supported with <b>prestd</b> option.
    -------------------------------------------------------------------------------------------------------------
    See <a href="#stlog-1-standardization">discussion of standardization</a>.

    <i>FE &amp; constant</i>         Description
    -------------------------------------------------------------------------------------------------------------
    <b>fe</b>                     within-transformation is applied prior to estimation. Requires data to be xtset.
    <b>noftools</b>               do not use <a href="#stlog-1-SG2016"><b>ftools</b></a> package for fixed-effects transform (slower; rarely used)
    <b><u>noc</u></b><b>onstant</b>             suppress constant from estimation.  Default behaviour is to partial the constant out
                            (i.e., to center the regressors).
    -------------------------------------------------------------------------------------------------------------

    <i>Optimization</i>          Description
    -------------------------------------------------------------------------------------------------------------
    <b><u>tolo</u></b><b>pt(</b><i>real</i><b>)</b>           tolerance for lasso shooting algorithm (default=1e-10)
    <b><u>tolz</u></b><b>ero(</b><i>real</i><b>)</b>          minimum below which coeffs are rounded down to zero (default=1e-4)
    <b><u>maxi</u></b><b>ter(</b><i>int</i><b>)</b>           maximum number of iterations for the lasso shooting algorithm (default=10,000)
    -------------------------------------------------------------------------------------------------------------

<a name="stlog-1-plottingopts"></a>    <i>Plotting options*</i>     Description
    -------------------------------------------------------------------------------------------------------------
    <b><u>plot</u></b><b>path(</b><i>method</i><b>)</b>       plots the coefficients path as a function of the L1-norm (<i>norm</i>), lambda (<i>lambda</i>) or
                            the log of lambda (<i>lnlambda</i>)
    <b><u>plotv</u></b><b>ar(</b><i>varlist</i><b>)</b>       list of variables to be included in the plot
    <b><u>ploto</u></b><b>pt(</b><i>string</i><b>)</b>        additional plotting options passed on to <a href="http://www.stata.com/help.cgi?line"><b>line</b></a>. For example, use <b>plotopt(legend(off))</b>
                            to turn off the legend.
    <b><u>plotl</u></b><b>abel</b>              displays variable labels in graph.
    -------------------------------------------------------------------------------------------------------------
    <i>*</i> Plotting is not available if lambda is a scalar value.

    <i>Display options</i>       Description
    -------------------------------------------------------------------------------------------------------------
    <b>displayall</b><i>*</i>            display full coefficient vectors including unselected variables (default: display only
                            selected, unpenalized and partialled-out)
    <b>postall</b><i>*</i>               post full coefficient vector including unselected variables in e(b) (default: e(b) has
                            only selected, unpenalized and partialled-out)
    <b>long</b>†                  show long output; instead of showing only the points at which predictors enter or
                            leave the model, all models are shown.
    <b><u>ver</u></b><b>bose</b>                show additional output
    <b><u>vver</u></b><b>bose</b>               show even more output
    <b>ic(</b><i>string</i><b>)</b>†            controls which information criterion is shown in the output.  'aic', 'bic', 'aicc',
                            and 'ebic' (the default' are allowed).  Note the lower case spelling.  See
                            <a href="#stlog-1-informationcriteria">Information criteria</a> for the definition of each information criterion.
    <b>noic</b>†                  suppresses the calculation of information criteria.  This will lead to speed gains if
                            alpha&lt;1, since calculation of effective degrees of freedom requires one inversion per
                            lambda.
    <b>wnorm</b>†                 displays L1 norm of beta estimates weighted by penalty loadings, i.e., ||Ups*beta||(1)
                            instead of ||beta||(1), which is the default.  Note that this also affects plotting
                            if <b><u>plot</u></b><b>path(</b><i>norm</i><b>)</b>} is specified.
    -------------------------------------------------------------------------------------------------------------
    <i>*</i> Only applicable if lambda is a scalar value.  † Only applicable if lambda is a list (the default).

    Replay syntax

        <b>lasso2</b> [<b>,</b> <b><u>plot</u></b><b>path(</b><i>method</i><b>)</b> <b><u>plotv</u></b><b>ar(</b><i>varlist</i><b>)</b> <b><u>ploto</u></b><b>pt(</b><i>string</i><b>)</b> <b><u>plotl</u></b><b>abel</b> <b>long</b> <b>postest</b> <b>lic(</b><i>string</i><b>)</b> <b>ic(</b><i>string</i><b>)</b>
              <b>wnorm</b>]

    <i>Replay options</i>        Description
    -------------------------------------------------------------------------------------------------------------
    <b>long</b>                   show long output; instead of showing only the points at which predictors enter or
                            leave the model, all models are shown.
    <b>ic(</b><i>string</i><b>)</b>             controls which information criterion is shown in the output.  'aic', 'bic', 'aicc',
                            and 'ebic' (the default) are allowed.  Note the lower case spelling.  See <a href="#stlog-1-informationcriteria">Information</a>
                            <a href="#stlog-1-informationcriteria">criteria</a> for the definition of each information criterion.
    <b>lic(</b><i>string</i><b>)</b>            estimate model corresponding to minimum information criterion.  'aic', 'bic', 'aicc',
                            and 'ebic' (the default) are allowed.  Note the lower case spelling.  See <a href="#stlog-1-informationcriteria">Information</a>
                            <a href="#stlog-1-informationcriteria">criteria</a> for the definition of each information criterion.
    <b><u>poste</u></b><b>st</b>                store estimation results in e() if <b>lic(</b><i>string</i><b>)</b> is used
    <b><u>plot</u></b><b>path(</b><i>method</i><b>)</b>       see <a href="#stlog-1-plottingopts">Plotting options</a> above
    <b><u>plotv</u></b><b>ar(</b><i>varlist</i><b>)</b>       see <a href="#stlog-1-plottingopts">Plotting options</a> above
    <b><u>ploto</u></b><b>pt(</b><i>string</i><b>)</b>        see <a href="#stlog-1-plottingopts">Plotting options</a> above
    <b><u>plotl</u></b><b>abel</b>              see <a href="#stlog-1-plottingopts">Plotting options</a> above
    -------------------------------------------------------------------------------------------------------------
    Only applicable if lambda was a list in the previous <b>lasso2</b> estimation.

    Postestimation:

        <b>predict</b> [<a href="http://www.stata.com/help.cgi?datatypes"><i>type</i></a>] <a href="http://www.stata.com/help.cgi?newvar"><i>newvar</i></a> [<a href="http://www.stata.com/help.cgi?if"><i>if</i></a>] [<a href="http://www.stata.com/help.cgi?in"><i>in</i></a>] [<b>,</b> <b>xb</b> <b><u>r</u></b><b>esiduals</b> <b>ols</b> <b><u>l</u></b><b>ambda(</b><i>real</i><b>)</b> <b>lid(</b><i>int</i><b>)</b> <b><u>appr</u></b><b>ox</b> <b><u>noi</u></b><b>sily</b> <b><u>poste</u></b><b>st</b>]

    <i>Predict options</i>       Description
    -------------------------------------------------------------------------------------------------------------
    <b>xb</b>                     compute predicted values (the default)
    <b><u>r</u></b><b>esiduals</b>              compute residuals
    <b>ols</b>                    use post-estimation OLS for prediction
    <b><u>l</u></b><b>ambda(</b><i>real</i><b>)</b>‡          lambda value for prediction. Ignored if <b>lasso2</b> was called with scalar lambda value.
    <b>lid(</b><i>int</i><b>)</b>‡              index of lambda value for prediction.
    <b><u>appr</u></b><b>ox</b>‡                linear approximation is used instead of re-estimation.  Faster, but only exact if
                            coefficient path is piecewise linear.
    <b><u>noi</u></b><b>sily</b>‡               show estimation output if re-estimation required
    <b><u>poste</u></b><b>st</b>‡               store estimation results in e() if re-estimation is used
    -------------------------------------------------------------------------------------------------------------
    ‡ Only applicable if lambda was a list in the previous <b>lasso2</b> estimation.

    <b>lasso2</b> may be used with time-series or panel data, in which case the data must be tsset or xtset first; see
    help <a href="http://www.stata.com/help.cgi?tsset"><b>tsset</b></a> or <a href="http://www.stata.com/help.cgi?xtset"><b>xtset</b></a>.

    All varlists may contain time-series operators or factor variables; see help varlist.

<b><u>Contents</u></b>

    <a href="#stlog-1-description">Description</a>
    <a href="#stlog-1-coordinate">Coordinate descent algorithm</a>
    <a href="#stlog-1-penalization">Penalization level</a>
    <a href="#stlog-1-standardization">Standardization of variables</a>
    <a href="#stlog-1-informationcriteria">Information criteria</a>
    <a href="#stlog-1-estimators">Estimators</a>
    <a href="#stlog-1-examples">Examples and demonstration</a>
    <a href="#stlog-1-examples_data">--Data set</a>
    <a href="#stlog-1-examples_general">--General demonstration</a>
    <a href="#stlog-1-examples_information">--Information criteria</a>
    <a href="#stlog-1-examples_plotting">--Plotting</a>
    <a href="#stlog-1-examples_predicted">--Predicted values</a>
    <a href="#stlog-1-examples_standardization">--Standardization</a>
    <a href="#stlog-1-examples_penalty">--Penalty loadings and notpen()</a>
    <a href="#stlog-1-examples_partialling">--Partialling vs penalization</a>
    <a href="#stlog-1-examples_adaptive">--Adaptive lasso</a>
    <a href="#stlog-1-saved_results">Saved results</a>
    <a href="#stlog-1-references">References</a>
    <a href="#stlog-1-website">Website</a>
    <a href="#stlog-1-acknowledgements">Acknowledgements</a>
    <a href="#stlog-1-citation">Citation of lassopack</a>


<a name="stlog-1-description"></a><b><u>Description</u></b>

    <b>lasso2</b> solves the following problem

        1/N RSS + lambda/N*alpha*||Ups*beta||[1] + lambda/(2*N)*(1-alpha)*||Ups*beta||[2], 
        
    where

    RSS        = sum(y(i)-x(i)'beta)^2 denotes the residual sum of squares,
    beta       is a p-dimensional parameter vector,
    lambda     is the overall penalty level,
    ||.||[j]   denotes the L(j) vector norm for j=1,2;
    alpha      the elastic net parameter, which determines the relative contribution of L1 (lasso-type) to L2
                (ridge-type) penalization.
    Ups        is a p by p diagonal matrix of predictor-specific penalty loadings. Note that <b>lasso2</b> treats Ups as
                a row vector.
    N          number of observations

    Note: the above lambda differs from the definition used in parts of the lasso and elastic net literature; see
    for example the R package <i>glmnet</i> by Friedman et al. (<a href="#stlog-1-Friedman2010"><b>2010</b></a>).  We have here adopted an objective function
    following Belloni et al. (<a href="#stlog-1-Belloni2012"><b>2012</b></a>).  Specifically, <i>lambda=2*N*lambda(GN)</i> where <i>lambda(GN)</i> is the penalty level
    used by <i>glmnet</i>.

    In addition, if the option <b>sqrt</b> is specified, <b>lasso2</b> estimates the square-root lasso (sqrt-lasso) estimator,
    which is defined as the solution to the following objective function:

        sqrt(1/N*RSS) + lambda/N*||Ups*beta||[1]. 
        
<a name="stlog-1-coordinate"></a><b><u>Coordinate descent algorithm</u></b>

    <b>lasso2</b> implements the elastic net and sqrt-lasso using coordinate descent algorithms.  The algorithm (then
    referred to as "shooting") was first proposed by Fu (<a href="#stlog-1-Fu1998"><b>1998</b></a>) for the lasso, and by Van der Kooij (<a href="#stlog-1-Kooji2007"><b>2007</b></a>) for the
    elastic net.  Belloni et al. (<a href="#stlog-1-BelloniSqrt2011"><b>2011</b></a>) implement the coordinate descent for the sqrt-lasso, and have kindly
    provided Matlab code.

    Coordinate descent algorithms repeatedly cycle over predictors <i>j</i>=1,...,<i>p</i> and update single coefficient
    estimates until convergence.  Suppose the predictors are centered and standardized to have unit variance.  In
    that case, the update for coefficient <i>j</i> is obtained using univariate regression of the current partial
    residuals (i.e., excluding the contribution of predictor <i>j</i>) against predictor <i>j</i>.

    The algorithm requires an initial beta estimate for which the Ridge estimate is used.  If the coefficient
    path is obtained for a list of lambda values, <b>lasso2</b> starts from the largest lambda value and uses previous
    estimates as warm starts.

    See Friedman et al. (<a href="#stlog-1-Friedman2007"><b>2007</b></a>, <a href="#stlog-1-Friedman2010"><b>2010</b></a>), and references therein, for further information.

<a name="stlog-1-penalization"></a><b><u>Penalization level: choice of lambda (and alpha)</u></b>

    Penalized regression methods, such as the elastic net and the sqrt-lasso, rely on tuning parameters that
    control the degree and type of penalization.  The estimation methods implemented in <b>lasso2</b> use two tuning
    parameters:  lambda, which controls the general degree of penalization, and alpha, which determines the
    relative contribution of L1-type to L2-type penalization.  <b>lasso2</b> obtains elastic net and sqrt-lasso
    solutions for a given lambda value or a list of lambda values, and for a given alpha value (default=1).

    <b>lassopack</b> offers three approaches for selecting the "optimal" lambda (and alpha) value:

    (1) The penalty level may be chosen by cross-validation in order to optimize out-of-sample prediction
    performance.  <i>K</i>-fold cross-validation and rolling cross-validation (for panel and time-series data) are
    implemented in <a href="http://www.stata.com/help.cgi?cvlasso"><b>cvlasso</b></a>.  <b>cvlasso</b> also supports cross-validation across alpha.

    (2) Theoretically justified and feasible penalty levels and loadings are available for the lasso and
    sqrt-lasso via the separate command <a href="http://www.stata.com/help.cgi?rlasso"><b>rlasso</b></a>.  The penalization is chosen to dominate the noise of the
    data-generating process (represented by the score vector), which allows derivation of theoretical results
    with regard to consistent prediction and parameter estimation.  Since the error variance is in practice
    unknown, Belloni et al. (<a href="#stlog-1-Belloni2012"><b>2012</b></a>) introduce the rigorous (or feasible) lasso that relies on an iterative
    algorithm for estimating the optimal penalization and is valid in the presence of non-Gaussian and
    heteroskedastic errors.  Belloni et al. (<a href="#stlog-1-Belloni2016"><b>2016</b></a>) extend the framework to the panel data setting.  In the case
    of the sqrt-lasso under homoskedasticity, the optimal penalty level is independent of the unknown error
    variance, leading to a practical advantage and better performance in finite samples (see Belloni et al., 
    <a href="#stlog-1-BelloniSqrt2011"><b>2011</b></a>, <a href="#stlog-1-BelloniSqrt2014"><b>2014</b></a>).  See help <a href="http://www.stata.com/help.cgi?rlasso"><b>rlasso</b></a> for more details.

    (3) Lambda can also be selected using information criteria.  <b>lasso2</b> calculates four information criteria:
    Akaike Information Criterion (AIC; Akaike, <a href="#stlog-1-Akaike1974"><b>1974</b></a>), Bayesian Information Criterion (BIC; Schwarz, <a href="#stlog-1-Schwarz1978"><b>1978</b></a>),
    Extended Bayesian information criterion (EBIC; Chen &amp; Chen, <a href="#stlog-1-Chen2008"><b>2008</b></a>) and the corrected AIC (AICc; Sugiura, <a href="#stlog-1-Sugiura1978"><b>1978</b></a>,
    and Hurvich, <a href="#stlog-1-Hurvich1989"><b>1989</b></a>).  By default, <b>lasso2</b> displays EBIC in the output, but all four information criteria are
    stored in <b>e(aic)</b>, <b>e(bic)</b>, <b>e(ebic)</b> and <b>e(aicc)</b>.  See section <a href="#stlog-1-informationcriteria">Information criteria</a> for more information.

<a name="stlog-1-standardization"></a><b><u>Standardization of variables</u></b>

    Standard practice is for predictors to be "standardized", i.e., normalized to have mean zero and unit
    variance.  By default <b>lasso2</b> achieves this by incorporating the standardization into the penalty loadings.
    We refer to this method as standardization "on the fly", as standardization occurs during rather than before
    estimation.  Alternatively, the option <b>prestd</b> causes the predictors to be standardized prior to the
    estimation.

    Standardizing "on the fly" via the penalty loadings and pre-standardizing the data prior to estimation are
    theoretically equivalent.  The default standardizing "on the fly" is often faster.  The <b>prestd</b> option can
    lead to improved numerical precision or more stable results in the case of difficult problems; the cost is
    the computation time required to pre-standardize the data.

<a name="stlog-1-estimators"></a><b><u>Estimators</u></b>

    <u>Ridge regression (Hoerl &amp; Kennard, </u><a href="#stlog-1-Hoerl1970"><b><u>1970</u></b></a><u>)</u>

    The ridge estimator can be written as

         betahat(ridge) = (X'X+lambda*I(p))^(-1)X'y.

    Thus, even if X'X is not full rank (e.g. because p&gt;n), the problem becomes nonsingular by adding a constant
    to the diagonal of X'X.  Another advantage of the ridge estimator over least squares stems from the
    variance-bias trade-off.  Ridge regression may improve over ordinary least squares by inducing a mild bias
    while decreasing the variance.  For this reason, ridge regression is a popular method in the context of
    multicollinearity.  In contrast to estimators relying on L1-penalization, the ridge does not yield sparse
    solutions and keeps all predictors in the model.

    <u>Lasso estimator (Tibshirani, </u><a href="#stlog-1-Tib1996"><b><u>1996</u></b></a><u>)</u>

    The lasso minimizes the residual sum of squares subject to a constraint on the absolute size of coefficient
    estimates.  Tibshirani (<a href="#stlog-1-Tib1996"><b>1996</b></a>) motivates the lasso with two major advantages over least squares.  First, due
    to the nature of the L1-penalty, the lasso tends to produce sparse solutions and thus facilitates model
    interpretation.  Secondly, similar to ridge regression, lasso can outperform least squares in terms of
    prediction due to lower variance.  Another advantage is that the lasso is computationally attractive due to
    its convex form.  This is in contrast to model selection based on AIC or BIC (which employ L0 penalization)
    where each possible sub-model has to be estimated.

    <u>Elastic net (Zou &amp; Hastie, </u><a href="#stlog-1-Zou2005"><b><u>2005</u></b></a><u>)</u>

    The elastic net applies a mix of L1 (lasso-type) and L2 (ridge-type) penalization.  It combines some of the
    strengths of lasso and ridge regression.  In the presence of groups of correlated regressors, the lasso
    selects typically only one variable from each group, whereas the ridge tends to produce similar coefficient
    estimates for groups of correlated variables.  On the other hand, the ridge does not yield sparse solutions
    impeding model interpretation.  The elastic net is able to produce sparse solutions (for some alpha greater
    than zero) and retains (or drops) correlated variables jointly.

    <u>Adaptive lasso (Zou, </u><a href="#stlog-1-Zou2006"><b><u>2006</u></b></a><u>)</u>

    The lasso is only variable selection consistent under the rather strong "irrepresentable condition", which
    imposes constraints on the degree of correlation between predictors in the true model and predictors outside
    of the model (see Zhao &amp; Yu, <a href="#stlog-1-Zhao2006"><b>2006</b></a>; Meinshausen &amp; Bühlmann, <a href="#stlog-1-Buhlmann2006"><b>2006</b></a>).  Zou (<a href="#stlog-1-Zou2006"><b>2006</b></a>) proposes the adaptive lasso
    which uses penalty loadings of 1/abs(beta0(j))^theta where beta0 is an initial estimator.  The adaptive lasso
    is variable-selection consistent for fixed p under weaker assumptions than the standard lasso.  If p&lt;n, OLS
    can be used as the initial estimator.  Huang et al. (<a href="#stlog-1-Huang2008"><b>2008</b></a>) suggest to use univariate OLS if p&gt;n.  Other
    initial estimators are possible.

    <u>Square-root lasso (Belloni et al., </u><a href="#stlog-1-BelloniSqrt2011"><b><u>2011</u></b></a><u>, </u><a href="#stlog-1-BelloniSqrt2014"><b><u>2014</u></b></a><u>)</u>

    The sqrt-lasso is a modification of the lasso that minimizes (RSS)^(1/2) instead of RSS, while also imposing
    an L1-penalty.  The main advantage of the sqrt-lasso over the standard lasso is that the theoretically
    grounded, data-driven optimal lambda is independent of the unknown error variance under homoskedasticity.
    See <a href="http://www.stata.com/help.cgi?rlasso"><b>rlasso</b></a>.

    <u>Post-estimation OLS</u>

    Penalized regression methods induce a bias that can be alleviated by post-estimation OLS, which applies OLS
    to the predictors selected by the first-stage variable selection method.  For the case of the lasso, Belloni
    and Chernozhukov (<a href="#stlog-1-Belloni2013"><b>2013</b></a>) have shown that the post-lasso OLS performs at least as well as the lasso under mild
    additional assumptions.

    For further information on the lasso and related methods, see for example the textbooks by Hastie et al.
    (<a href="#stlog-1-Hastie2009"><b>2009</b></a>, <a href="#stlog-1-Hastie2015"><b>2015</b></a>; both available for free) and Bühlmann &amp; Van de Geer (<a href="#stlog-1-Buhlmann2011"><b>2011</b></a>).

<a name="stlog-1-informationcriteria"></a><b><u>Information criteria</u></b>
 
    The information criteria supported by <b>lasso2</b> are the Akaike information criterion (AIC, Akaike, <a href="#stlog-1-Akaike1974"><b>1974</b></a>), the
    Bayesian information criterion (BIC, Schwarz, <a href="#stlog-1-Schwarz1978"><b>1978</b></a>), the corrected AIC (Sugiura, <a href="#stlog-1-Sugiura1978"><b>1978</b></a>; Hurvich, <a href="#stlog-1-Hurvich1989"><b>1989</b></a>), and
    the Extended BIC (Chen &amp; Chen, <a href="#stlog-1-Chen2008"><b>2008</b></a>).  These are given by (omitting dependence on lambda and alpha):

        AIC     = N*log(RSS/N) + 2*<i>df</i>
        BIC     = N*log(RSS/N) + <i>df</i>*log(N) 
        AICc    = N*log(RSS/N) + 2*<i>df</i>*N/(N-<i>df</i>)
        EBIC    = BIC + 2*<i>gamma</i>*<i>df</i>*log(p)

    where RSS(lambda,alpha) is the residual sum of squares and <i>df</i>(lambda,alpha) is the effective degrees of
    freedom, which is a measure of model complexity.  In the linear regression model, the degrees of freedom is
    simply the number of regressors.  Zou et al. (<a href="#stlog-1-Zou2007"><b>2007</b></a>) show that the number of non-zero coefficients is an
    unbiased and consistent estimator of <i>df</i>(lambda,alpha) for the lasso.  More generally, the degrees of freedom
    of the elastic net can be calculated as the trace of the projection matrix.  With an unbiased estimator for
    <i>df</i> available, the above information criteria can be employed to select tuning parameters.

    The BIC is known to be model selection consistent if the true model is among the candidate models, whereas
    the AIC tends to yield an overfitted model.  On the other hand, the AIC is loss efficient in the sense that
    it selects the model that minimizes the squared average prediction error, while the BIC does not possess this
    property.  Zhang et al. (<a href="#stlog-1-Zhang2010"><b>2010</b></a>) show that these principles also apply when AIC and BIC are employed to select
    the tuning parameter for penalized regression.

    Both AIC and BIC tend to overselect regressors in the small-N-large-p case.  The AICc corrects the small
    sample bias of the AIC which can be especially severe in the high-dimensional context. Similarily, the EBIC
    addresses the shortcomings of the BIC when p is large by imposing a larger penalty on the number of
    coefficients.  Chen &amp; Chen (<a href="#stlog-1-Chen2008"><b>2008</b></a>) show that the EBIC performs better in terms of false discovery rate at the
    cost of a negligible reduction in the positive selection rate.

    The EBIC depends on an additional parameter, <i>gamma</i>, which can be controlled using <b><u>ebicg</u></b><b>amma(</b><i>real</i><b>)</b>.  <i>gamma</i>=0
    is equivalent to the BIC.  We follow Chen &amp; Chen (<a href="#stlog-1-Chen2008"><b>2008</b></a>, p. 768) and use <i>gamma</i>=1-log(n)/(2*log(p)) as the
    default choice.  An upper and lower threshold is applied to ensure that <i>gamma</i> lies in the [0,1] interval.

    The EBIC is displayed in the output of <b>lasso2</b> by default (if lambda is a list), but all four information
    criteria are returned in <b>e()</b>.  The lambda values that minimize the information criteria for a given alpha are
    returned in <b>e(laic)</b>, <b>e(lbic)</b>, <b>e(laicc)</b> and <b>e(lebic)</b>, respectively.  To change the default display, use the
    <b>ic(</b><i>string</i><b>)</b> option.  <b>noic</b> suppresses the calculation of information criteria, which leads to a speed gain if
    alpha&lt;1.

<a name="stlog-1-examples"></a><b><u>Example using prostate cancer data (Stamey et al., </u></b><a href="#stlog-1-Stamey1989"><b><u>1989</u></b></a><b><u>)</u></b>

<a name="stlog-1-examples_data"></a>    <u>Data set</u>

    The data set is available through Hastie et al. (<a href="#stlog-1-Hastie2009"><b>2009</b></a>) on the authors' website.  The following variables are
    included in the data set of 97 men:

    Predictors    
      lcavol    log(cancer volume)
      lweight   log(prostate weight)
      age       patient age
      lbph      log(benign prostatic hyperplasia amount)
      svi       seminal vesicle invasion
      lcp       log(capsular penetration)
      gleason   Gleason score
      pgg45     percentage Gleason scores 4 or 5

    Outcome       
      lpsa      log(prostate specific antigen)

    Load prostate cancer data.
        . insheet using https://web.stanford.edu/~hastie/ElemStatLearn/datasets/prostate.data, clear tab

<a name="stlog-1-examples_general"></a>    <u>General demonstration</u>

    Estimate coefficient lasso path over (default) list of lambda values.
        . lasso2 lpsa lcavol lweight age lbph svi lcp gleason pgg45

    The replay syntax can be used to re-display estimation results.
        . lasso2

    User-specified lambda list.
        . lasso2 lpsa lcavol lweight age lbph svi lcp gleason pgg45, lambda(100 50 10)

    The list of returned <b>e()</b> objects depends on whether <b>lambda()</b> is a list (the default) or a scalar value.  For
    example, if lambda is a scalar, one vector of coefficient estimates is returned.  If lambda is a list, the
    whole coefficient path for a range of lambda values is obtained.  The last row of <b>e(betas)</b> is equal to the
    row vector <b>e(b)</b>.
        . lasso2 lpsa lcavol lweight age lbph svi lcp gleason pgg45, lambda(100 50 10)
        . ereturn list
        . mat list e(betas)
        . lasso2 lpsa lcavol lweight age lbph svi lcp gleason pgg45, lambda(10)
        . ereturn list
        . mat list e(b)

    Sqrt-lasso.
        . lasso2 lpsa lcavol lweight age lbph svi lcp gleason pgg45, sqrt

    Ridge regression.  All predictors are included in the model.
        . lasso2 lpsa lcavol lweight age lbph svi lcp gleason pgg45, alpha(0)

    Elastic net with alpha=0.1.  Even though alpha is close to zero (Ridge regression), the elastic net can
    produce sparse solutions.
        . lasso2 lpsa lcavol lweight age lbph svi lcp gleason pgg45, alpha(0.1)

    The option <b>ols</b> triggers the use of post-estimation OLS.  OLS alleviates the shrinkage bias induced by L1 and
    L2 norm penalization.
        . lasso2 lpsa lcavol lweight age lbph svi lcp gleason pgg45, ols
        . lasso2 lpsa lcavol lweight age lbph svi lcp gleason pgg45, sqrt ols
        . lasso2 lpsa lcavol lweight age lbph svi lcp gleason pgg45, alpha(0.1) ols

<a name="stlog-1-examples_information"></a>    <u>Information criteria</u>

    <b>lasso2</b> calculates four information criteria: AIC, BIC, EBIC and AICc.  The EBIC is shown by default in the
    output along with the R-squared.
        . lasso2 lpsa lcavol lweight age lbph svi lcp gleason pgg45

    To see another information criterion in the outout, use the <b>ic(</b><i>string</i><b>)</b> option where <i>string</i> can be replaced by
    <i>aic</i>, <i>bic</i>, <i>ebic</i> or <i>aicc</i> (note the lower case spelling).  For example, to display AIC:
        . lasso2 lpsa lcavol lweight age lbph svi lcp gleason pgg45, ic(aic)

    In fact, there is no need to re-run the full model.  We can make use of the replay syntax:
        . lasso2, ic(aic)

    The <b>long</b> option triggers extended output; instead of showing only the points at which predictors enter or
    leave the model, all models are shown.  An asterisk marks the model (i.e., the value of lambda) that
    minimizes the information criterion (here, AIC).
        . lasso2, ic(aic) long

    To estimate the model corresponding to the minimum information criterion, click on the link at the bottom of
    the output or type one of the following:
        . lasso2, lic(aic)
        . lasso2, lic(ebic)
        . lasso2, lic(bic)
        . lasso2, lic(aicc)

    To store the estimation results of the selected model, add the <b>postest</b> option.
        . lasso2, lic(ebic)
        . ereturn list
        . lasso2, lic(ebic) postest
        . ereturn list

    The same can also be achieved in one line without using the replay syntax.  <b>lasso2</b> first obtains the full
    coefficient path for a list lambda values, and then runs the model selected by AIC.  Again, <b>postest</b> can be
    used to store results of the selected model.
        . lasso2 lpsa lcavol lweight age lbph svi lcp gleason pgg45, lic(aic) postest

<a name="stlog-1-examples_plotting"></a>    <u>Plotting</u>

    Plot coefficients against lambda:  As lambda increases, the coefficient estimates are shrunk towards zero.
    Lambda=0 corresponds to OLS and if lambda is sufficiently large the model is empty.
        . lasso2 lpsa lcavol lweight age lbph svi lcp gleason pgg45, plotpath(lambda)

    Plot coefficients against L1 norm.
        . lasso2 lpsa lcavol lweight age lbph svi lcp gleason pgg45, plotpath(norm)

    The replay syntax can also be used for plotting.
        . lasso2, plotpath(norm)

    Only selected variables are plotted.
        . lasso2, plotpath(norm) plotvar(lcavol svi)

    The variable names can be displayed directly next to each series using <b>plotlabel</b>.  <b>plotopt(legend(off))</b>
    suppresses the legend.
        . lasso2, plotpath(lambda) plotlabel plotopt(legend(off))
        . lasso2, plotpath(norm) plotlabel plotopt(legend(off))

<a name="stlog-1-examples_predicted"></a>    <u>Predicted values</u>

    <b>xbhat1</b> is generated by re-estimating the model for lambda=10.  The <b>noisily</b> option triggers the display of the
    estimation results.  <b>xbhat2</b> is generated by linear approximation using the two beta estimates closest to
    lambda=10.
        . lasso2 lpsa lcavol lweight age lbph svi lcp gleason pgg45
        . cap drop xbhat1
        . predict double xbhat1, xb l(10) noisily
        . cap drop xbhat2
        . predict double xbhat2, xb l(10) approx

    The model is estimated explicitly using lambda=10.  If <b>lasso2</b> is called with a scalar lambda value, the
    subsequent <b>predict</b> command requires no <b>lambda()</b> option.
        . lasso2 lpsa lcavol lweight age lbph svi lcp gleason pgg45, lambda(10)
        . cap drop xbhat3
        . predict double xbhat3, xb

    All three methods yield the same results.  However note that the linear approximation is only exact for the
    lasso which is piecewise linear.
        . sum xbhat1 xbhat2 xbhat3

    It is also possible to obtain predicted values by referencing a specific lambda ID using the <b>lid()</b> option.
        . lasso2 lpsa lcavol lweight age lbph svi lcp gleason pgg45
        . cap drop xbhat4
        . predict double xbhat4, xb lid(21)
        . cap drop xbhat5
        . predict double xbhat5, xb l(25.45473900468241)
        . sum xbhat4 xbhat5

<a name="stlog-1-examples_standardization"></a>    <u>Standardization</u>

    By default <b>lasso2</b> standardizes the predictors to have unit variance.  Standardization is done by default "on
    the fly" via penalty loadings.  The coefficient estimates are returned in original units.
        . lasso2 lpsa lcavol lweight age lbph svi lcp gleason pgg45, l(10)

    Instead of standardizing "on the fly" by setting penalization loadings equal to standardization loadings, we
    can standardize the regressors prior to estimation with the <b>prestd</b> option.  Both methods are equivalent in
    theory.  Standardizing "on the fly" tends to be faster, but pre-standardization may lead to more stable
    results in the case of difficult problems.  See <a href="#stlog-1-standardization">here</a> for more information.
        . lasso2 lpsa lcavol lweight age lbph svi lcp gleason pgg45, l(10)
        . mat list e(Ups)
        . lasso2 lpsa lcavol lweight age lbph svi lcp gleason pgg45, l(10) prestd
        . mat list e(Ups)

    The used penalty loadings are stored in <b>e(Ups)</b>.  In the first case above, the standardization loadings are
    returned.  In the second case the penalty loadings are equal to one for all regressors.

    To get the coefficients in standard deviation units, <b>stdcoef</b> can be specified along with the <b>prestd</b> option.
        . lasso2 lpsa lcavol lweight age lbph svi lcp gleason pgg45, l(10) prestd stdcoef

    We can override any form of standardization with the <b>unitloadings</b> options, which sets the penalty loadings to
    a vector of 1s.
        . lasso2 lpsa lcavol lweight age lbph svi lcp gleason pgg45, l(10) unitloadings

    The same logic applies to the sqrt-lasso (and elastic net).
        . lasso2 lpsa lcavol lweight age lbph svi lcp gleason pgg45, l(10) sqrt
        . lasso2 lpsa lcavol lweight age lbph svi lcp gleason pgg45, l(10) sqrt prestd

<a name="stlog-1-examples_penalty"></a>    <u>Penalty loadings and </u><b><u>notpen()</u></b>

    By default the penalty loading vector is a vector of standard deviations.
        . lasso2 lpsa lcavol lweight age lbph svi lcp gleason pgg45, l(10)
        . mat list e(Ups)

    We can set the penalty loading for specific predictors to zero, implying no penalization.  Unpenalized
    predictor are always included in the model.
        . lasso2 lpsa lcavol lweight age lbph svi lcp gleason pgg45, l(10) notpen(lcavol)
        . mat list e(Ups)

    We can specify custom penalty loadings.  The option <b>ploadings</b> expects a row vector of size <i>p</i> where <i>p</i> is the
    number of regressors (excluding the constant, which is partialled out).  Because we pre-standardize the data
    (and we are using the lasso) the results are equivalent to the results above (standardizing on the fly and
    specifying lcavol as unpenalized).
        . mat myloadings = (0,1,1,1,1,1,1,1)
        . lasso2 lpsa lcavol lweight age lbph svi lcp gleason pgg45, l(10) ploadings(myloadings) prestd
        . mat list e(Ups)

<a name="stlog-1-examples_partialling"></a>    <u>Partialling vs penalization</u>

    If lambda and the penalty loadings are kept constant, partialling out and not penalizing of variables yields
    the same results for the included/penalized regressors.  Yamada (<a href="#stlog-1-Yamada2017"><b>2017</b></a>) shows that the equivalence of
    partialling out and not penalizing holds for lasso and ridge regression.  The examples below suggest that the
    same result also holds for the elastic net in general and the sqrt-lasso.  Note that the equivalence only
    holds if the regressor matrix and other penalty loadings are the same.  Below we use the <b>unitloadings</b> option
    to achieve this; alternatively we could use the <b>ploadings(</b>.<b>)</b> option.

    Lasso.
        . lasso2 lpsa lcavol lweight age lbph svi lcp gleason pgg45, l(10) notpen(lcavol) unitload
        . lasso2 lpsa lcavol lweight age lbph svi lcp gleason pgg45, l(10) partial(lcavol) unitload

    Sqrt-lasso.
        . lasso2 lpsa lcavol lweight age lbph svi lcp gleason pgg45, l(10) sqrt notpen(lcavol) unitload
        . lasso2 lpsa lcavol lweight age lbph svi lcp gleason pgg45, l(10) sqrt partial(lcavol) unitload

    Ridge regression.
        . lasso2 lpsa lcavol lweight age lbph svi lcp gleason pgg45, l(10) alpha(0) notpen(lcavol) unitload
        . lasso2 lpsa lcavol lweight age lbph svi lcp gleason pgg45, l(10) alpha(0) partial(lcavol) unitload

    Elastic net.
        . lasso2 lpsa lcavol lweight age lbph svi lcp gleason pgg45, l(10) alpha(0.5) notpen(lcavol) unitload
        . lasso2 lpsa lcavol lweight age lbph svi lcp gleason pgg45, l(10) alpha(0.5) partial(lcavol) unitload

<a name="stlog-1-examples_adaptive"></a>    <u>Adaptive lasso</u>

    The adaptive lasso relies on an initial estimator to calculate the penalty loadings.  The penalty loadings
    are given by 1/abs(beta0(j))^theta, where beta0(j) denotes the initial estimate for predictor j.  By default,
    <b>lasso2</b> uses OLS as the initial estimator as originally suggested by Zou (<a href="#stlog-1-Zou2006"><b>2006</b></a>).  If the number of parameters
    exceeds the numbers of observations, univariate OLS is used; see Huang et al. (<a href="#stlog-1-Huang2008"><b>2008</b></a>).
        . lasso2 lpsa lcavol lweight age lbph svi lcp gleason pgg45, adaptive
        . mat list e(Ups)

    See the OLS estimates for comparison.
        . reg lpsa lcavol lweight age lbph svi lcp gleason pgg45

    Theta (the exponent for calculating the adaptive loadings) can be changed using the <b>adatheta()</b> option.
        . lasso2 lpsa lcavol lweight age lbph svi lcp gleason pgg45, adaptive adat(2)
        . mat list e(Ups)

    Other initial estimators such as ridge regression are possible.
        . lasso2 lpsa lcavol lweight age lbph svi lcp gleason pgg45, l(10) alpha(0)
        . mat bhat_ridge = e(b)
        . lasso2 lpsa lcavol lweight age lbph svi lcp gleason pgg45, adaptive adaloadings(bhat_ridge)
        . mat list e(Ups)

<a name="stlog-1-saved_results"></a><b><u>Saved results</u></b>

    The set of returned e-class objects depends on whether lambda is a scalar or a list (the default).

    scalars       
      <b>e(N)</b>               sample size
      <b>e(cons)</b>            =1 if constant is present, 0 otherwise
      <b>e(fe)</b>              =1 if fixed effects model is used, 0 otherwise
      <b>e(alpha)</b>           elastic net parameter
      <b>e(sqrt)</b>            =1 if the sqrt-lasso is used, 0 otherwise
      <b>e(ols)</b>             =1 if post-estimation OLS results are returned, 0 otherwise
      <b>e(adaptive)</b>        =1 if adaptive loadings are used, 0 otherwise
      <b>e(p)</b>               number of penalized regressors in model
      <b>e(notpen_ct)</b>       number of unpenalized variables
      <b>e(partial_ct)</b>      number of partialled out regressors (incl constant)
      <b>e(prestd)</b>          =1 if pre-standardized
      <b>e(lcount)</b>          number of lambda values

    scalars (only if lambda is a list)
      <b>e(lmax)</b>            largest lambda value
      <b>e(lmin)</b>            smallest lambda value

    scalars (only if lambda is a scalar)
      <b>e(lambda)</b>          penalty level
      <b>e(rmse)</b>            root mean squared error
      <b>e(rmseOLS)</b>         root mean squared error of post-estimation OLS
      <b>e(k)</b>               number of selected and unpenalized/partialled-out regressors including constant (if
                           present)
      <b>e(s)</b>               number of selected regressors
      <b>e(s0)</b>              number of selected and unpenalized regressors including constant (if present)
      <b>e(niter)</b>           number of iterations
      <b>e(maxiter)</b>         maximum number of iterations
      <b>e(tss)</b>             total sum of squares
      <b>e(aicmin)</b>          minimum AIC
      <b>e(bicmin)</b>          minimum BIC
      <b>e(aiccmin)</b>         minimum AICc
      <b>e(ebicmin)</b>         minimum EBIC
      <b>e(laic)</b>            lambda corresponding to minimum AIC
      <b>e(lbic)</b>            lambda corresponding to minimum BIC
      <b>e(laicc)</b>           lambda corresponding to minimum AICc
      <b>e(lebic)</b>           lambda corresponding to minimum EBIC

    macros        
      <b>e(cmd)</b>             command name
      <b>e(depvar)</b>          name of dependent variable
      <b>e(varX)</b>            all predictors
      <b>e(varXmodel)</b>       penalized predictors
      <b>e(partial)</b>         partialled out predictors
      <b>e(notpen)</b>          unpenalized predictors
      <b>e(method)</b>          estimation method

    macros (only if lambda is a scalar)
      <b>e(selected)</b>        selected predictors
      <b>e(selected0)</b>       selected predictors excluding constant

    matrices      
      <b>e(Ups)</b>             row vector used penalty loadings
      <b>e(stdvec)</b>          row vector of standardization loadings

    matrices (only if lambda is a list)
      <b>e(lambdamat)</b>       column vector of lambdas
      <b>e(l1norm)</b>          column vector of L1 norms for each lambda value (excludes the intercept)
      <b>e(wl1norm)</b>         column vector of weighted L1 norms for each lambda value (excludes the intercept), see
                           <b>wnorm</b>
      <b>e(dof)</b>             column vector of L0 norm for each lambda value (excludes the intercept)
      <b>e(betas)</b>           matrix of estimates, where each row corresponds to one lambda value. The intercept is
                           stored in the last column.
      <b>e(ess)</b>             column vector of explained sum of squares for each lambda value
      <b>e(rss)</b>             column vector of residual sum of squares for each lambda value
      <b>e(ebic)</b>            column vector of EBIC for each lambda value
      <b>e(bic)</b>             column vector of BIC for each lambda value
      <b>e(aic)</b>             column vector of AIC for each lambda value
      <b>e(aicc)</b>            column vector of AICc for each lambda value
      <b>e(rsq)</b>             column vector of R-squared for each lambda value

    matrices (only if lambda is a scalar)
      <b>e(b)</b>               posted coefficient vector (see <b>postall</b> and <b>displayall</b>). Used for prediction.
      <b>e(beta)</b>            coefficient vector
      <b>e(betaOLS)</b>         coefficient vector of post-estimation OLS
      <b>e(betaAll)</b>         full coefficient vector including omitted, factor base variables, etc.
      <b>e(betaAllOLS)</b>      full post-estimation OLS coefficient vector including omitted, factor base variables,
                           etc.

    functions     
      <b>e(sample)</b>          estimation sample

<a name="stlog-1-references"></a><b><u>References</u></b>

<a name="stlog-1-Akaike1974"></a>    Akaike, H. (1974). A new look at the statistical model identification. <i>IEEE Transactions on Automatic</i>
        <i>Control</i>, 19(6), 716–723.  https://doi.org/10.1109/TAC.1974.1100705

<a name="stlog-1-BelloniSqrt2011"></a>    Belloni, A., Chernozhukov, V., &amp; Wang, L. (2011).  Square-root lasso: pivotal recovery of sparse signals via
        conic programming.  <i>Biometrika</i> 98(4), 791–806.  https://doi.org/10.1093/biomet/asr043

<a name="stlog-1-Belloni2012"></a>    Belloni, A., Chen, D., Chernozhukov, V., &amp; Hansen, C. (2012). Sparse Models and Methods for Optimal
        Instruments With an Application to Eminent Domain. <i>Econometrica</i> 80(6), 2369–2429.  
        https://doi.org/10.3982/ECTA9626

<a name="stlog-1-Belloni2013"></a>    Belloni, A., &amp; Chernozhukov, V. (2013). Least squares after model selection in high-dimensional sparse
        models. <i>Bernoulli</i>, 19(2), 521–547.  https://doi.org/10.3150/11-BEJ410

<a name="stlog-1-BelloniSqrt2014"></a>    Belloni, A., Chernozhukov, V., &amp; Wang, L. (2014). Pivotal estimation via square-root Lasso in nonparametric
        regression. <i>The Annals of Statistics</i> 42(2), 757–788.  https://doi.org/10.1214/14-AOS1204

<a name="stlog-1-Belloni2016"></a>    Belloni, A., Chernozhukov, V., Hansen, C., &amp; Kozbur, D. (2016). Inference in High Dimensional Panel Models
        with an Application to Gun Control. <i>Journal of Business &amp; Economic Statistics</i> 34(4), 590–605.
        Methodology.  https://doi.org/10.1080/07350015.2015.1102733

<a name="stlog-1-Buhlmann2006"></a>    Bühlmann, P., &amp; Meinshausen, N. (2006). High-dimensional graphs and variable selection with the Lasso.
        {it:The Annals of Statistics], 34(3), 1436–1462.  http://doi.org/10.1214/009053606000000281

<a name="stlog-1-Buhlmann2011"></a>    Bühlmann, P., &amp; Van de Geer, S. (2011). Statistics for High-Dimensional Data. Berlin, Heidelberg:
        Springer-Verlag.

<a name="stlog-1-Chen2008"></a>    Chen, J., &amp; Chen, Z. (2008). Extended Bayesian information criteria for model selection with large model
        spaces. <i>Biometrika</i>, 95(3), 759–771.  https://doi.org/10.1093/biomet/asn034

<a name="stlog-1-SG2016"></a>    Correia, S. 2016.  FTOOLS: Stata module to provide alternatives to common Stata commands optimized for large
        datasets.  https://ideas.repec.org/c/boc/bocode/s458213.html

<a name="stlog-1-Fu1998"></a>    Fu, W. J. (1998). Penalized Regressions: The Bridge Versus the Lasso. <i>Journal of Computational and Graphical</i>
        <i>Statistics</i> 7(3), 397–416.  https://doi.org/10.2307/1390712

<a name="stlog-1-Friedman2007"></a>    Friedman, J., Hastie, T., Höfling, H., &amp; Tibshirani, R. (2007). Pathwise coordinate optimization. <i>The Annals</i>
        <i>of Applied Statistics</i> 1(2), 302–332.  https://doi.org/10.1214/07-AOAS131

<a name="stlog-1-Friedman2010"></a>    Friedman, J., Hastie, T., &amp; Tibshirani, R. (2010). Regularization Paths for Generalized Linear Models via
        Coordinate Descent. <i>Journal of Statistical Software</i> 33(1), 1–22.  https://doi.org/10.18637/jss.v033.i01

<a name="stlog-1-Hastie2009"></a>    Hastie, T., Tibshirani, R., &amp; Friedman, J. (2009). The Elements of Statistical Learning (2nd ed.). New York:
        Springer-Verlag.  https://web.stanford.edu/~hastie/ElemStatLearn/

<a name="stlog-1-Hastie2015"></a>    Hastie, T., Tibshirani, R., &amp; Wainwright, M. J. (2015). Statistical Learning with Sparsity: The Lasso and
        Generalizations. Boca Raton: CRC Press, Taylor &amp; Francis.  
        https://www.stanford.edu/~hastie/StatLearnSparsity/

<a name="stlog-1-Hoerl1970"></a>    Hoerl, A. E., &amp; Kennard, R. W. (1970). Ridge Regression: Applications to Nonorthogonal Problems.
        <i>Technometrics</i> 12(1), 69–82.  https://doi.org/10.1080/00401706.1970.10488635

<a name="stlog-1-Huang2008"></a>    Huang, J., Ma, S., &amp; Zhang, C.-H. (2008). Adaptive Lasso for Sparse High-Dimensional Regression Models
        Supplement. <i>Statistica Sinica</i> 18, 1603–1618.  https://doi.org/10.2307/24308572

<a name="stlog-1-Hurvich1989"></a>    Hurvich, C. M., &amp; Tsai, C.-L. (1989). Regression and time series model selection in small samples.
        <i>Biometrika</i>, 76(2), 297–307.  http://doi.org/10.1093/biomet/76.2.297

<a name="stlog-1-Schwarz1978"></a>    Schwarz, G. (1978). Estimating the Dimension of a Model. <i>The Annals of Statistics</i>, 6(2), 461–464.  
        https://doi.org/10.1214/aos/1176344136

<a name="stlog-1-Stamey1989"></a>    Stamey, T. A., Kabalin, J. N., Mcneal, J. E., Johnstone, I. M., Freiha, F., Redwine, E. A., &amp; Yang, N.
        (1989). Prostate Specific Antigen in the Diagnosis and Treatment of Adenocarcinoma of the Prostate. II.
        Radical Prostatectomy Treated Patients. <i>The Journal of Urology</i> 141(5), 1076–1083. 
        https://doi.org/10.1016/S0022-5347(17)41175-X

<a name="stlog-1-Sugiura1978"></a>    Sugiura, N. (1978). Further analysts of the data by akaike’ s information criterion and the finite
        corrections. <i>Communications in Statistics - Theory and Methods</i>, 7(1), 13–26.  
        http://doi.org/10.1080/03610927808827599

<a name="stlog-1-Tib1996"></a>    Tibshirani, R. (1996). Regression Shrinkage and Selection via the Lasso. <i>Journal of the Royal Statistical</i>
        <i>Society. Series B (Methodological)</i> 58(1), 267–288.  https://doi.org/10.2307/2346178

<a name="stlog-1-Kooji2007"></a>    Van der Kooij A (2007). Prediction Accuracy and Stability of Regrsssion with Optimal Scaling Transformations.
        Ph.D. thesis, Department of Data Theory, University of Leiden.  http://hdl.handle.net/1887/12096

<a name="stlog-1-Yamada2017"></a>    Yamada, H. (2017). The Frisch–Waugh–Lovell theorem for the lasso and the ridge regression. <i>Communications in</i>
        <i>Statistics - Theory and Methods</i> 46(21), 10897–10902.  https://doi.org/10.1080/03610926.2016.1252403

<a name="stlog-1-Zhang2010"></a>    Zhang, Y., Li, R., &amp; Tsai, C.-L. (2010). Regularization Parameter Selections via Generalized Information
        Criterion. <i>Journal of the American Statistical Association</i>, 105(489), 312–323.  
        http://doi.org/10.1198/jasa.2009.tm08013

<a name="stlog-1-Zhao2006"></a>    Zhao, P., &amp; Yu, B. (2006). On Model Selection Consistency of Lasso. <i>Journal of Machine Learning Research</i>, 7,
        2541–2563.  http://dl.acm.org/citation.cfm?id=1248547.1248637

<a name="stlog-1-Zou2005"></a>    Zou, H., &amp; Hastie, T. (2005). Regularization and variable selection via the elastic net. <i>Journal of the Royal</i>
        <i>Statistical Society. Series B: Statistical Methodology</i> 67(2), 301–320.  
        https://doi.org/10.1111/j.1467-9868.2005.00503.x

<a name="stlog-1-Zou2006"></a>    Zou, H. (2006). The Adaptive Lasso and Its Oracle Properties. <i>Journal of the American Statistical Association</i>
        101(476), 1418–1429.  https://doi.org/10.1198/016214506000000735

<a name="stlog-1-Zou2007"></a>    Zou, H., Hastie, T., &amp; Tibshirani, R. (2007). On the "degrees of freedom" of the lasso. Ann. Statist., 35(5),
        2173–2192.  https://doi.org/10.1214/009053607000000127

<a name="stlog-1-website"></a><b><u>Website</u></b>

    Please check our website https://statalasso.github.io/ for more information.

<a name="stlog-1-installation"></a><b><u>Installation</u></b>

    To get the latest stable version of <i>lassopack</i> from our website, check the installation instructions at 
    https://statalasso.github.io/installation/.  We update the stable website version more frequently than the
    SSC version.

<a name="stlog-1-acknowledgements"></a><b><u>Acknowledgements</u></b>

    Thanks to Alexandre Belloni, who provided Matlab code for the square-root lasso estimator, Sergio Correia for
    advice on the use of the FTOOLS package, and Jan Ditzen.


<a name="stlog-1-citation"></a><b><u>Citation of lasso2</u></b>

    <b>lasso2</b> is not an official Stata command. It is a free contribution to the research community, like a paper.
    Please cite it as such:

    Ahrens, A., Hansen, C.B., Schaffer, M.E. 2018.  lasso2: Program for lasso, square-root lasso, elastic net,
        ridge, adaptive lasso and post-estimation OLS.  http://ideas.repec.org/c/boc/bocode/s458458.html


<b><u>Authors</u></b>

        Achim Ahrens, Economic and Social Research Institute, Ireland
        achim.ahrens@esri.ie
        
        Christian B. Hansen, University of Chicago, USA
        Christian.Hansen@chicagobooth.edu

</pre>

        
      </section>

      <footer class="page__meta">
        
        


        
      </footer>

      

      
    </div>

    
  </article>

  
  
</div>
    </div>

    

    <div class="page__footer">
      <footer>
        <!-- start custom footer snippets -->

<!-- end custom footer snippets -->
        <div class="page__footer-follow">
  <ul class="social-icons">
    
      <li><strong>Follow:</strong></li>
    
    
    
    
    
    
    <li><a href="/feed.xml"><i class="fas fa-fw fa-rss-square" aria-hidden="true"></i> Feed</a></li>
  </ul>
</div>

<div class="page__footer-copyright">&copy; 2018 Achim Ahrens. Powered by <a href="https://jekyllrb.com" rel="nofollow">Jekyll</a> &amp; <a href="https://mademistakes.com/work/minimal-mistakes-jekyll-theme/" rel="nofollow">Minimal Mistakes</a>.</div>

      </footer>
    </div>

    
  <script src="/assets/js/main.min.js"></script>
  <script src="https://use.fontawesome.com/releases/v5.0.9/js/all.js"></script>








  </body>
</html>