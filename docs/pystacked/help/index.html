<!DOCTYPE html>
<html lang="en" dir="ltr">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1.0">
<meta name="description" content="------------------------------------------------------------------------------------------------------------------------ help pystacked v0.4.8 ------------------------------------------------------------------------------------------------------------------------ Title pystacked -- Stata program for Stacking Regression Overview pystacked implements stacking regression (Wolpert, 1992) via scikit-learn&#39;s sklearn.ensemble.StackingRegressor and sklearn.ensemble.StackingClassifier. Stacking is a way of combining multiple supervised machine learners (the &#34;base&#34; or &#34;level-0&#34; learners) into a meta learner. The currently supported base learners are linear regression, logit, lasso, ridge, elastic net, (linear) support vector machines, gradient boosting, and neural nets (MLP). pystacked can also be used with a single base learner and, thus, provides an easy-to-use API for scikit-learn&#39;s machine learning algorithms.">
<meta name="theme-color" content="#FFFFFF">
<meta name="color-scheme" content="light dark"><meta property="og:title" content="Help file" />
<meta property="og:description" content="------------------------------------------------------------------------------------------------------------------------ help pystacked v0.4.8 ------------------------------------------------------------------------------------------------------------------------ Title pystacked -- Stata program for Stacking Regression Overview pystacked implements stacking regression (Wolpert, 1992) via scikit-learn&#39;s sklearn.ensemble.StackingRegressor and sklearn.ensemble.StackingClassifier. Stacking is a way of combining multiple supervised machine learners (the &#34;base&#34; or &#34;level-0&#34; learners) into a meta learner. The currently supported base learners are linear regression, logit, lasso, ridge, elastic net, (linear) support vector machines, gradient boosting, and neural nets (MLP). pystacked can also be used with a single base learner and, thus, provides an easy-to-use API for scikit-learn&#39;s machine learning algorithms." />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://statalasso.github.io/docs/pystacked/help/" /><meta property="article:section" content="docs" />



<title>Help file | Stata ML Page</title>
<link rel="manifest" href="/manifest.json">
<link rel="icon" href="/favicon.png" type="image/x-icon">
<link rel="stylesheet" href="/book.min.faa17d5e23166cd4e013165a396fc44278cb9136b0672f68e8ff906d4e5b956b.css" integrity="sha256-&#43;qF9XiMWbNTgExZaOW/EQnjLkTawZy9o6P&#43;QbU5blWs=" crossorigin="anonymous">
  <script defer src="/flexsearch.min.js"></script>
  <script defer src="/en.search.min.84ed2095db797254e48f89b2a512861df48e8828d1cd34696cd090a9af6dd4ce.js" integrity="sha256-hO0gldt5clTkj4mypRKGHfSOiCjRzTRpbNCQqa9t1M4=" crossorigin="anonymous"></script>
<!--
Made with Book Theme
https://github.com/alex-shpak/hugo-book
-->
  
</head>
<body dir="ltr">
  <input type="checkbox" class="hidden toggle" id="menu-control" />
  <input type="checkbox" class="hidden toggle" id="toc-control" />
  <main class="container flex">
    <aside class="book-menu">
      <div class="book-menu-content">
        
  <nav>
<h2 class="book-brand">
  <a class="flex align-center" href="/"><span>Stata ML Page</span>
  </a>
</h2>


<div class="book-search">
  <input type="text" id="book-search-input" placeholder="Search" aria-label="Search" maxlength="64" data-hotkeys="s/" />
  <div class="book-search-spinner hidden"></div>
  <ul id="book-search-results"></ul>
</div>












  



  
  <ul>
    
      
        <li class="book-section-flat" >
          
  
  

  
    <input type="checkbox" id="section-a28c195407e662cdbffd5df0c1ccdd3c" class="toggle"  />
    <label for="section-a28c195407e662cdbffd5df0c1ccdd3c" class="flex justify-between">
      <a href="https://statalasso.github.io/docs/lassopack/" class="">LASSOPACK</a>
    </label>
  

          
  <ul>
    
      
        <li>
          
  
  

  
    <a href="https://statalasso.github.io/docs/lassopack/package_overview/" class="">Package overview</a>
  

        </li>
      
    
      
        <li>
          
  
  

  
    <a href="https://statalasso.github.io/docs/lassopack/estimators/" class="">Estimation methods</a>
  

        </li>
      
    
      
        <li>
          
  
  

  
    <a href="https://statalasso.github.io/docs/lassopack/regularized_reg/" class="">Regularized regression</a>
  

        </li>
      
    
      
        <li>
          
  
  

  
    <a href="https://statalasso.github.io/docs/lassopack/lasso2/" class="">Getting started</a>
  

        </li>
      
    
      
        <li>
          
  
  

  
    <a href="https://statalasso.github.io/docs/lassopack/cvlasso/" class="">Cross-validation</a>
  

        </li>
      
    
      
        <li>
          
  
  

  
    <a href="https://statalasso.github.io/docs/lassopack/rlasso/" class="">Rigorous lasso</a>
  

        </li>
      
    
      
        <li>
          
  
  

  
    <a href="https://statalasso.github.io/docs/lassopack/python_support/" class="">Python support</a>
  

        </li>
      
    
      
        <li>
          
  
  

  
    <input type="checkbox" id="section-7b22f9a619ffc04f2040b80267ff8ec1" class="toggle"  />
    <label for="section-7b22f9a619ffc04f2040b80267ff8ec1" class="flex justify-between">
      <a href="https://statalasso.github.io/docs/lassopack/lassologit/" class="">Lassologit</a>
    </label>
  

          
  <ul>
    
      
        <li>
          
  
  

  
    <a href="https://statalasso.github.io/docs/lassopack/lassologit/lassologit_demo/" class="">Example using Spam data</a>
  

        </li>
      
    
  </ul>

        </li>
      
    
      
        <li>
          
  
  

  
    <a href="https://statalasso.github.io/docs/lassopack/lasso2_replication/" class="">Comparison glmnet</a>
  

        </li>
      
    
      
        <li>
          
  
  

  
    <input type="checkbox" id="section-de33cd2a5faa36dd8a4f9fdad33eadc5" class="toggle"  />
    <label for="section-de33cd2a5faa36dd8a4f9fdad33eadc5" class="flex justify-between">
      <a role="button" class="">Help files</a>
    </label>
  

          
  <ul>
    
      
        <li>
          
  
  

  
    <a href="https://statalasso.github.io/docs/lassopack/help/lasso2_help/" class="">help lasso2</a>
  

        </li>
      
    
      
        <li>
          
  
  

  
    <a href="https://statalasso.github.io/docs/lassopack/help/cvlasso_help/" class="">help cvlasso</a>
  

        </li>
      
    
      
        <li>
          
  
  

  
    <a href="https://statalasso.github.io/docs/lassopack/help/rlasso_help/" class="">help  rlasso</a>
  

        </li>
      
    
      
    
  </ul>

        </li>
      
    
      
        <li>
          
  
  

  
    <a href="https://statalasso.github.io/docs/lassopack/installation/" class="">Installation</a>
  

        </li>
      
    
      
        <li>
          
  
  

  
    <a href="https://statalasso.github.io/docs/lassopack/lassopack_cite/" class="">Citation</a>
  

        </li>
      
    
  </ul>

        </li>
      
    
      
        <li class="book-section-flat" >
          
  
  

  
    <input type="checkbox" id="section-1660971b512dcfeb0bcc54343ae1329f" class="toggle"  />
    <label for="section-1660971b512dcfeb0bcc54343ae1329f" class="flex justify-between">
      <a href="https://statalasso.github.io/docs/pdslasso/" class="">PDSLASSO</a>
    </label>
  

          
  <ul>
    
      
        <li>
          
  
  

  
    <a href="https://statalasso.github.io/docs/pdslasso/pdslasso_models/" class="">Models</a>
  

        </li>
      
    
      
        <li>
          
  
  

  
    <a href="https://statalasso.github.io/docs/pdslasso/pdslasso_demo/" class="">Demonstration</a>
  

        </li>
      
    
      
        <li>
          
  
  

  
    <a href="https://statalasso.github.io/docs/pdslasso/pdslasso_panel/" class="">Panel FE</a>
  

        </li>
      
    
      
        <li>
          
  
  

  
    <a href="https://statalasso.github.io/docs/pdslasso/ivlasso_help/" class="">Help file</a>
  

        </li>
      
    
      
        <li>
          
  
  

  
    <a href="https://statalasso.github.io/docs/pdslasso/installation/" class="">Installation</a>
  

        </li>
      
    
      
        <li>
          
  
  

  
    <a href="https://statalasso.github.io/docs/pdslasso/pdslasso_cite/" class="">Citation</a>
  

        </li>
      
    
  </ul>

        </li>
      
    
      
        <li class="book-section-flat" >
          
  
  

  
    <input type="checkbox" id="section-a6d0313d37b86997de2e3c697c4d2888" class="toggle" checked />
    <label for="section-a6d0313d37b86997de2e3c697c4d2888" class="flex justify-between">
      <a href="https://statalasso.github.io/docs/pystacked/" class="">PYSTACKED</a>
    </label>
  

          
  <ul>
    
      
        <li>
          
  
  

  
    <a href="https://statalasso.github.io/docs/pystacked/getting_started/" class="">Getting started</a>
  

        </li>
      
    
      
        <li>
          
  
  

  
    <a href="https://statalasso.github.io/docs/pystacked/regression/" class="">Regression</a>
  

        </li>
      
    
      
        <li>
          
  
  

  
    <a href="https://statalasso.github.io/docs/pystacked/classification/" class="">Classification</a>
  

        </li>
      
    
      
        <li>
          
  
  

  
    <a href="https://statalasso.github.io/docs/pystacked/parallel/" class="">Parallelization</a>
  

        </li>
      
    
      
        <li>
          
  
  

  
    <a href="https://statalasso.github.io/docs/pystacked/help/" class=" active">Help file</a>
  

        </li>
      
    
      
        <li>
          
  
  

  
    <a href="https://statalasso.github.io/docs/pystacked/installation/" class="">Installation</a>
  

        </li>
      
    
      
        <li>
          
  
  

  
    <a href="https://statalasso.github.io/docs/pystacked/citation/" class="">Citation</a>
  

        </li>
      
    
  </ul>

        </li>
      
    
      
        <li class="book-section-flat" >
          
  
  

  
    <input type="checkbox" id="section-72f375ffaf85eea4cd43789c74047530" class="toggle"  />
    <label for="section-72f375ffaf85eea4cd43789c74047530" class="flex justify-between">
      <a href="https://statalasso.github.io/docs/ddml/" class="">DDML</a>
    </label>
  

          
  <ul>
    
      
        <li>
          
  
  

  
    <a href="https://statalasso.github.io/docs/ddml/models/" class="">Model overview</a>
  

        </li>
      
    
      
        <li>
          
  
  

  
    <a href="https://statalasso.github.io/docs/ddml/crossfit/" class="">Algorithm</a>
  

        </li>
      
    
      
        <li>
          
  
  

  
    <a href="https://statalasso.github.io/docs/ddml/plm/" class="">Partial Linear Model (PLM)</a>
  

        </li>
      
    
      
        <li>
          
  
  

  
    <a href="https://statalasso.github.io/docs/ddml/plm2/" class="">PLM &amp; Stacking</a>
  

        </li>
      
    
      
        <li>
          
  
  

  
    <a href="https://statalasso.github.io/docs/ddml/interactive/" class="">Interactive</a>
  

        </li>
      
    
      
        <li>
          
  
  

  
    <a href="https://statalasso.github.io/docs/ddml/iv/" class="">Partial Linear IV</a>
  

        </li>
      
    
      
        <li>
          
  
  

  
    <a href="https://statalasso.github.io/docs/ddml/ivhd/" class="">Flexible IV</a>
  

        </li>
      
    
      
        <li>
          
  
  

  
    <a href="https://statalasso.github.io/docs/ddml/interactiveiv/" class="">Interactive IV</a>
  

        </li>
      
    
      
        <li>
          
  
  

  
    <a href="https://statalasso.github.io/docs/ddml/help/" class="">ddml help file</a>
  

        </li>
      
    
      
        <li>
          
  
  

  
    <a href="https://statalasso.github.io/docs/ddml/help_qddml/" class="">qddml help file</a>
  

        </li>
      
    
      
        <li>
          
  
  

  
    <a href="https://statalasso.github.io/docs/ddml/installation/" class="">Installation</a>
  

        </li>
      
    
  </ul>

        </li>
      
    
      
        <li>
          
  
  

  
    <a href="https://statalasso.github.io/docs/python/" class="">Install Stata/Python</a>
  

          
  <ul>
    
  </ul>

        </li>
      
    
      
        <li>
          
  
  

  
    <a href="https://statalasso.github.io/docs/about/" class="">About</a>
  

          
  <ul>
    
  </ul>

        </li>
      
    
      
        <li>
          
  
  

  
    <a href="https://statalasso.github.io/docs/papers/" class="">Readings</a>
  

          
  <ul>
    
  </ul>

        </li>
      
    
  </ul>















</nav>




  <script>(function(){var e=document.querySelector("aside .book-menu-content");addEventListener("beforeunload",function(){localStorage.setItem("menu.scrollTop",e.scrollTop)}),e.scrollTop=localStorage.getItem("menu.scrollTop")})()</script>


 
      </div>
    </aside>

    <div class="book-page">
      <header class="book-header">
        
  <div class="flex align-center justify-between">
  <label for="menu-control">
    <img src="/svg/menu.svg" class="book-icon" alt="Menu" />
  </label>

  <strong>Help file</strong>

  <label for="toc-control">
    
  </label>
</div>


  
 
      </header>

      
      
  <article class="markdown">
<pre id="stlog-1" style="font-size: 11px" class="sthlp">------------------------------------------------------------------------------------------------------------------------
<b>help pystacked</b>                                                                                                    v0.4.8
------------------------------------------------------------------------------------------------------------------------

<b><u>Title</u></b>

    <b>pystacked</b> --  Stata program for Stacking Regression

<b><u>Overview</u></b>

    <b>pystacked</b> implements stacking regression (<a href="#stlog-1-Wolpert1992"><b>Wolpert, 1992</b></a>) via scikit-learn's sklearn.ensemble.StackingRegressor and
    sklearn.ensemble.StackingClassifier.  Stacking is a way of combining multiple supervised machine learners (the
    "base" or "level-0" learners) into a meta learner.  The currently supported base learners are linear regression,
    logit, lasso, ridge, elastic net, (linear) support vector machines, gradient boosting, and neural nets (MLP).

    <b>pystacked</b> can also be used with a single base learner and, thus, provides an easy-to-use API for scikit-learn's
    machine learning algorithms.

    <b>pystacked</b> requires at least Stata 16 (or higher), a Python installation and scikit-learn (0.24 or higher).
    <b>pystacked</b> has been tested with scikit-learn 0.24, 1.0, 1.1.0, 1.1.1 and 1.1.2.  See <a href="http://www.stata.com/help.cgi?python"><b>here</b></a> and here for how to set
    up Python for Stata on your system.

<a name="stlog-1-methodopts"></a><b><u>Contents</u></b>

        <a href="#stlog-1-syntax_overview"><b>Syntax overview</b></a>
        <a href="#stlog-1-syntax1"><b>Syntax 1</b></a>
        <a href="#stlog-1-syntax2"><b>Syntax 2</b></a>
        <a href="#stlog-1-otheropts"><b>Other options</b></a>
        <a href="#stlog-1-postestimation"><b>Postestimation and prediction options</b></a>
        <a href="#stlog-1-section_stacking"><b>Stacking</b></a>
        <a href="#stlog-1-base_learners"><b>Supported base learners</b></a>
        <a href="#stlog-1-base_learners_opt"><b>Base learners: Options</b></a>
        <a href="#stlog-1-pipelines"><b>Pipelines</b></a>
        <a href="#stlog-1-predictors"><b>Learner-specific predictors</b></a>
        <a href="#stlog-1-example_boston"><b>Example Stacking Regression</b></a>
        <a href="#stlog-1-example_spam"><b>Example Stacking Classification</b></a>
        <a href="#stlog-1-installation"><b>Installation</b></a>
        <a href="#stlog-1-misc"><b>Misc (references, contact, etc.)</b></a>

<a name="stlog-1-syntax_overview"></a><b><u>Syntax overview</u></b>

    There are two alternative syntaxes. The <u>first syntax</u> is:

        <b>pystacked</b> <i>depvar</i> <i>predictors</i> [<b>if</b> <i>exp</i>] [<b>in</b> <i>range</i>] [<b>,</b> <b>methods(</b><i>string</i><b>)</b> <b>cmdopt1(</b><i>string</i><b>)</b> <b>cmdopt2(</b><i>string</i><b>)</b> <b>...</b>
              <b>pipe1(</b><i>string</i><b>)</b> <b>pipe2(</b><i>string</i><b>)</b> <b>...</b>  <b>xvars1(</b><i>varlist</i><b>)</b> <b>xvars2(</b><i>varlist</i><b>)</b> <b>...</b>  <a href="#stlog-1-otheropts"><i>otheropts</i></a> ]

    The <u>second syntax</u> is:

        <b>pystacked</b> <i>depvar</i> <i>predictors</i> || <b><u>m</u></b><b>ethod(</b><i>string</i><b>)</b> <b>opt(</b><i>string</i><b>)</b> <b><u>pipe</u></b><b>line(</b><i>string</i><b>)</b> <b>xvars(</b><i>varlist</i><b>)</b> || <b><u>m</u></b><b>ethod(</b><i>string</i><b>)</b>
              <b>opt(</b><i>string</i><b>)</b> <b><u>pipe</u></b><b>line(</b><i>string</i><b>)</b> <b>xvars(</b><i>varlist</i><b>)</b> || <b>...</b>  || [<b>if</b> <i>exp</i>] [<b>in</b> <i>range</i>] [<b>,</b> <a href="#stlog-1-otheropts"><i>otheropts</i></a> ]

    The first syntax uses <b>methods(</b><i>string</i><b>)</b> to select base learners, where <i>string</i> is a list of base learners.  Options
    are passed on to base learners via <b>cmdopt1(</b><i>string</i><b>)</b>, <b>cmdopt2(</b><i>string</i><b>)</b> to <b>cmdopt10(</b><i>string</i><b>)</b>.  That is, up to 10 base
    learners can be specified and options are passed on in the order in which they appear in <b>methods(</b><i>string</i><b>)</b> (see
    <a href="#stlog-1-base_learners_opt"><b>Command options</b></a>).  Likewise, the <b>pipe*(</b><i>string</i><b>)</b> option can be used for pre-processing predictors within Python on
    the fly (see <a href="#stlog-1-pipelines"><b>Pipelines</b></a>).  Furthermore, <b>xvars*(</b><i>varlist</i><b>)</b> allows to specify a learner-specific varlist of predictors.

    The second syntax imposes no limit on the number of base learners (aside from the increasing computational
    complexity). Base learners are added before the comma using <b>method(</b><i>string</i><b>)</b> together with <b>opt(</b><i>string</i><b>)</b> and separated
    by "||".

<a name="stlog-1-syntax1"></a><b><u>Syntax 1</u></b>

    <i>Option</i>                Description
    ------------------------------------------------------------------------------------------------------------------
    <b>methods(</b><i>string</i><b>)</b>        a list of base learners, defaults to "<i>ols lassocv gradboost</i>" for regression and "<i>logit</i>
                            <i>lassocv gradboost</i>" for classification; see <a href="#stlog-1-base_learners"><b>Base learners</b></a>.
    <b>cmdopt*(</b><i>string</i><b>)</b>        options passed to the base learners, see <a href="#stlog-1-base_learners_opt"><b>Command options</b></a>.
    <b>pipe*(</b><i>string</i><b>)</b>          pipelines passed to the base learners, see <a href="#stlog-1-pipelines"><b>Pipelines</b></a>.  Regularized linear learners use the
                            <i>stdscaler</i> pipeline by default, which standardizes the predictors. To suppress this, use
                            <i>nostdscaler</i>.  For other learners, there is no default pipeline.
    <b>xvars*(</b><i>varlist</i><b>)</b>        overwrites the default list of predictors.  That is, you can specify learner-specific lists
                            of predictors.  See <a href="#stlog-1-predictors"><b>here</b></a>.
    ------------------------------------------------------------------------------------------------------------------
    <i>Note:</i> <b>*</b> is replaced with 1 to 10. The number refers to the order given in <b>methods(</b><i>string</i><b>)</b>.

<a name="stlog-1-syntax2"></a><b><u>Syntax 2</u></b>

    <i>Option</i>                Description
    ------------------------------------------------------------------------------------------------------------------
    <b><u>m</u></b><b>ethod(</b><i>string</i><b>)</b>         a base learner, see <a href="#stlog-1-base_learners"><b>Base learners</b></a>.
    <b>opt(</b><i>string</i><b>)</b>            options, see <a href="#stlog-1-base_learners_opt"><b>Command options</b></a>.
    <b><u>pipe</u></b><b>line(</b><i>string</i><b>)</b>       pipelines applied to the predictors, see <a href="#stlog-1-pipelines"><b>Pipelines</b></a>.  pipelines passed to the base learners,
                            see <a href="#stlog-1-pipelines"><b>Pipelines</b></a>.  Regularized linear learners use the <i>stdscaler</i> pipeline by default, which
                            standardizes the predictors. To suppress this, use <i>nostdscaler</i>.  For other learners, there
                            is no default pipeline.
    <b>xvars(</b><i>varlist</i><b>)</b>         overwrites the default list of predictors.  That is, you can specify learner-specific lists
                            of predictors.  See <a href="#stlog-1-predictors"><b>here</b></a>.
    ------------------------------------------------------------------------------------------------------------------

<a name="stlog-1-otheropts"></a><b><u>Other options</u></b>

    <i>Option</i>                Description
    ------------------------------------------------------------------------------------------------------------------
    <b>type(</b><i>string</i><b>)</b>           <i>reg(ress)</i> for regression problems or <i>class(ify)</i> for classification problems.  The default
                            is regression.
    <b><u>final</u></b><b>est(</b><i>string</i><b>)</b>       final estimator used to combine base learners.  The default is non-negative least squares
                            without an intercept and the additional constraint that weights sum to 1 (<i>nnls1</i>).
                            Alternatives are <i>nnls0</i> (non-negative least squares without intercept without the
                            sum-to-one constraint), <i>singlebest</i> (use base learner with minimum MSE), <i>ols</i> (ordinary
                            least squares) or <i>ridge</i> for (logistic) ridge, which is the sklearn default. For more
                            information, see <a href="#stlog-1-section_stacking"><b>here</b></a>.
    <b><u>nosavep</u></b><b>red</b>             do not save predicted values (do not use if <b>predict</b> is used after estimation)
    <b><u>nosaveb</u></b><b>asexb</b>           do not save predicted values of each base learner (do not use if <b>predict</b> with <b><u>base</u></b><b>xb</b> is
                            used after estimation)
    <b>njobs(</b><i>int</i><b>)</b>             number of jobs for parallel computing. The default is 0 (no parallelization), -1 uses all
                            available CPUs, -2 uses all CPUs minus 1.
    <b>backend(</b><i>string</i><b>)</b>        joblib backend used for parallelization; the default is 'loky' under Linux/MacOS and
                            'threading' under Windows.  See here for more information.
    <b>folds(</b><i>int</i><b>)</b>             number of folds used for cross-validation (not relevant for voting); default is 5. Ignored
                            if <b>foldvar(</b><i>varname</i><b>)</b> if specified.
    <b>foldvar(</b><i>varname</i><b>)</b>       integer fold variable for cross-validation.
    <b>bfolds(</b><i>int</i><b>)</b>            number of folds used for <i>base learners</i> that use cross-validation (e.g. <i>lassocv</i>); default is
                            5.
    <b>norandom</b>               folds are created using the ordering of the data.
    <b>noshuffle</b>              cross-validation folds for <i>base learners</i> that use cross-validation (e.g. <i>lassocv</i>) are based
                            on ordering of the data.
    <b>sparse</b>                 converts predictor matrix to a sparse matrix. This will only lead to speed improvements if
                            the predictor matrix is sufficiently sparse. Not all learners support sparse matrices and
                            not all learners will benefit from sparse matrices in the same way. You can also use the
                            sparse pipeline to use sparse matrices for some learners, but not for others.
    <b>pyseed(</b><i>int</i><b>)</b>            set the Python seed. Note that since <b>pystacked</b> uses Python, we also need to set the Python
                            seed to ensure replicability.  Three options: 1) <b>pyseed(</b><i>-1</i><b>)</b> draws a number between 0 and
                            10^8 in Stata which is then used as a Python seed.  This way, you only need to deal with
                            the Stata seed. For example, <b>set seed 42</b> is sufficient, as the Python seed is generated
                            automatically. 2) Setting <b>pyseed(</b><i>x</i><b>)</b> with any positive integer <i>x</i> allows to control the
                            Python seed directly. 3) <b>pyseed(</b><i>0</i><b>)</b> sets the seed to None in Python.  The default is
                            <b>pyseed(</b><i>-1</i><b>)</b>.
    ------------------------------------------------------------------------------------------------------------------

    <i>Voting</i>                Description
    ------------------------------------------------------------------------------------------------------------------
    <b>voting</b>                 use voting regressor (ensemble.VotingRegressor) or voting classifier
                            (ensemble.VotingClassifier); see <a href="#stlog-1-section_stacking"><b>here</b></a> for a brief explanation.
    <b><u>votet</u></b><b>ype(</b><i>string</i><b>)</b>       type of voting classifier:  <i>hard</i> (default) or <i>soft</i>
    <b><u>votew</u></b><b>eights(</b><i>numlist</i><b>)</b>   positive weights used for voting regression/classification.  The length of <i>numlist</i> should
                            be the number of base learners - 1. The last weight is calculated to ensure that
                            sum(weights)=1.
    ------------------------------------------------------------------------------------------------------------------

<a name="stlog-1-postestimation"></a><b><u>Postestimation and prediction options</u></b>

    <u>Postestimation tables</u>

    After estimation, <b>pystacked</b> can report a table of in-sample (both cross-validated and full-sample-refitted) and,
    optionally, out-of-sample (holdout sample) performance for both the stacking regression and the base learners.
    For regression problems, the table reports the root MSPE (mean squared prediction error); for classification
    problems, a confusion matrix is reported.  The default holdout sample used for out-of-sample performance with the
    <b>holdout</b> option is all observations not included in the estimation.  Alternatively, the user can specify the
    holdout sample explicitly using the syntax <b>holdout(</b><i>varname</i><b>)</b>.  The table can be requested postestimation as below,
    or as part of the <b>pystacked</b> estimation command.

    Table syntax:

        <b>pystacked</b> [<b>,</b> <b><u>tab</u></b><b>le</b> <b>holdout</b>[<b>(</b><i>varname</i><b>)</b>] ]

    <u>Postestimation graphs</u>

    <b>pystacked</b> can also report graphs of in-sample and, optionally, out-of-sample (holdout sample) performance for both
    the stacking regression and the base learners.  For regression problems, the graphs compare predicted vs actual
    values of <i>depvar</i>.  For classification problems, the default is to report ROC curves; optionally, histograms of
    predicted probabilities are reported.  As with the <b>table</b> option, the default holdout sample used for out-of-sample
    performance is all observations not included in the estimation, but the user can instead specify the holdout
    sample explicitly.  The table can be requested postestimation as below, or as part of the <b>pystacked</b> estimation
    command.

    The <b>graph</b> option on its own reports the graphs using <b>pystacked</b>'s default settings.  Because graphs are produced
    using Stata's <a href="http://www.stata.com/help.cgi?twoway"><b>twoway</b></a>, <a href="http://www.stata.com/help.cgi?roctab"><b>roctab</b></a> and <a href="http://www.stata.com/help.cgi?histogram"><b>histogram</b></a> commands, the user can control either the combined graph
    (<b>graph(</b><i>options</i><b>)</b>) or the individual learner graphs (<b>lgraph(</b><i>options</i><b>)</b>) appear by passing options to these commands.

    Graph syntax:

        <b>pystacked</b> [<b>,</b> <b>graph</b>[<b>(</b><i>options</i><b>)</b>] <b>lgraph</b>[<b>(</b><i>options</i><b>)</b>] <b><u>hist</u></b><b>ogram</b> <b>holdout</b>[<b>(</b><i>varname</i><b>)</b>] ]

    <u>Prediction</u>

    To get stacking predicted values:

        <b>predict</b> <i>type</i> <i>newname</i> [<b>if</b> <i>exp</i>] [<b>in</b> <i>range</i>] [<b>,</b> <b>pr</b> <b>class</b> <b>xb</b> <b>resid</b> ]

    To get fitted values for each base learner:

        <b>predict</b> <i>type</i> <i>stub</i> [<b>if</b> <i>exp</i>] [<b>in</b> <i>range</i>] [<b>,</b> <b><u>base</u></b><b>xb</b> <b><u>cv</u></b><b>alid</b> ]

    <i>Option</i>                Description
    ------------------------------------------------------------------------------------------------------------------
    <b>xb</b>                     predicted value; the default for regression
    <b>pr</b>                     predicted probability; the default for classification
    <b>class</b>                  predicted class
    <b>resid</b>                  residuals
    <b><u>base</u></b><b>xb</b>                 predicted values for each base learner (default = use base learners re-fitted on full
                            estimation sample)
    <b><u>cv</u></b><b>alid</b>                 cross-validated predicted values. Currently only supported if combined with <b><u>base</u></b><b>xb</b>.
    ------------------------------------------------------------------------------------------------------------------

    <i>Note:</i> Predicted values (in and out-sample) are calculated and stored in Python memory when <b>pystacked</b> is run.
    <b>predict</b> pulls the predicted values from Python memory and saves them in Stata memory. This means that no changes
    on the data in Stata memory should be made <i>between</i> <b>pystacked</b> call and <b>predict</b> call. If changes to the data set are
    made, <b>predict</b> will return an error.

<a name="stlog-1-section_stacking"></a><b><u>Stacking</u></b>

    Stacking is a way of combining cross-validated predictions from multiple base ("level-0") learners into a final
    prediction. A final estimator ("level-1") is used to combine the base predictions.

    The default final predictor for stacking regession is non-negative least squares (NNLS) without an intercept and
    with the constraint that weights sum to one.  Note that in this respect we deviate from the scikit-learn default
    and follow the recommendation in Breiman (1996) and Hastie et al. (<a href="#stlog-1-Hastie2009"><b>2009</b></a>, p. 290).  The scikit-learn defaults for
    the final estimator are ridge regression for stacking regression and logistic ridge for classification tasks.  To
    use the scikit-learn default, use <b><u>final</u></b><b>est(</b><i>ridge</i><b>)</b>.  <b>pystacked</b> also supports ordinary (unconstrained) least squares
    as the final estimator (<b><u>final</u></b><b>est(</b><i>ols</i><b>)</b>).  Finally, <i>singlebest</i> uses the single base learner that exhibits the
    smallest cross-validated mean squared error.

    An alternative to stacking is voting. Voting regression uses the weighted average of base learners to form
    predictions. By default, the unweighted average is used, but the user can specify weights using
    <b><u>votew</u></b><b>eights(</b><i>numlist</i><b>)</b>. Voting classifier uses a majority rule by default (hard voting). An alternative is soft
    voting where the (weighted) probabilities are used to form the final prediction.

<a name="stlog-1-base_learners"></a><b><u>Supported base learners</u></b>

    The following base learners are supported:

    Base learners           
      <i>ols</i>                   Linear regression <i>(regression only)</i>
      <i>logit</i>                 Logistic regression <i>(classification only)</i>
      <i>lassoic</i>               Lasso with penalty chosen by AIC/BIC <i>(regression only)</i>
      <i>lassocv</i>               Lasso with cross-validated penalty
      <i>ridgecv</i>               Ridge with cross-validated penalty
      <i>elasticcv</i>             Elastic net with cross-validated penalty
      <i>svm</i>                   Support vector machines
      <i>gradboost</i>             Gradient boosting
      <i>rf</i>                    Random forest
      <i>linsvm</i>                Linear SVM
      <i>nnet</i>                  Neural net

    The base learners can be chosen using the <b>methods(</b><i>lassocv gradboost nnet</i><b>)</b> (Syntax 1) or <b><u>m</u></b><b>ethod(</b><i>string</i><b>)</b> options
    (Syntax 2).

    Please see links in the next section for more information on each method.

<a name="stlog-1-base_learners_opt"></a><b><u>Base learners: Options</u></b>

    Options can be passed to the base learners via <b>cmdopt*(</b><i>string</i><b>)</b> (Syntax 1) or <b>opt(</b><i>string</i><b>)</b> (Syntax 2).  The defaults
    are adopted from scikit-learn.

    To see the default options of each base learners, simply click on the "Show options" links below. To see which
    alternative settings are allowed, please see the scikit-learn documentations linked below.  We <i>strongly recommend</i>
    that you read the scikit-learn documentation carefully.

    The option <b><u>showopt</u></b><b>ions</b> shows the options passed on to Python.  We recommend to verify that options have been
    passed to Python as intended.

    <u>Linear regression</u>
    Methods <i>ols</i>
    <i>Type:</i> <i>reg</i>
    <i>Documentation:</i> linear_model.LinearRegression

    Show options

    <u>Logistic regression</u>
    Methods: <i>logit</i>
    Type: <i>class</i>
    Documentation: linear_model.LogisticRegression

    Show options

    <u>Penalized regression with information criteria</u>
    Methods <i>lassoic</i>
    Type: <i>reg</i>
    Documentation: linear_model.LassoLarsIC

    Show options

    <u>Penalized regression with cross-validation</u>

      Methods: <i>lassocv</i>, <i>ridgecv</i> and <i>elasticv</i>
      Type: <i>regress</i>
      Documentation: linear_model.ElasticNetCV

    Show lasso options
    Show ridge options
    Show elastic net options

    <u>Penalized logistic regression with cross-validation</u>
    Methods: <i>lassocv</i>, <i>ridgecv</i> and <i>elasticv</i>
    Type: <i>class</i>
    Documentation: linear_model.LogisticRegressionCV

    Show lasso options
    Show ridge options
    Show elastic options

    <u>Random forest classifier</u>
    Method: <i>rf</i>
    Type: <i>class</i>
    Documentation: ensemble.RandomForestClassifier

    Show options

    <u>Random forest regressor</u>
    Method: <i>rf</i>
    Type: <i>reg</i>
    Documentation: ensemble.RandomForestRegressor

    Show options

    <u>Gradient boosted regression trees</u>
    Method: <i>gradboost</i>
    Type: <i>reg</i>
    Documentation: ensemble.GradientBoostingRegressor

    Show options

    <u>Linear SVM (SVR)</u>
    Method: <i>linsvm</i>
    Type: <i>reg</i>
    Documentation: svm.LinearSVR

    Show options

    <u>SVM (SVR)</u>
    Method: <i>svm</i>
    Type: <i>class</i>
    Documentation: svm.SVR

    Show options

    <u>SVM (SVC)</u>
    Method: <i>svm</i>
    Type: <i>reg</i>
    Documentation: svm.SVC

    Show options

    <u>Neural net classifier (Multi-layer Perceptron)</u>
    Method: <i>nnet</i>
    Type: <i>class</i>
    Documentation: sklearn.neural_network.MLPClassifier

    Show options

    <u>Neural net regressor (Multi-layer Perceptron)</u>
    Method: <i>nnet</i>
    Type: <i>reg</i>
    Documentation: sklearn.neural_network.MLPRegressor

    Show options

<a name="stlog-1-predictors"></a><b><u>Learner-specific predictors</u></b>

    By default, <b>pystacked</b> uses the same set of predictors for all base learners. This is often not desirable. For
    example, when using linear machine learners such as the lasso, it is recommended to create interactions. There are
    two methods to allow for learner-specific sets of predictors:

    1) Pipelines, discussed in the next section, can be used to create polynomials on the fly.

    2) The <b>xvars*(</b><i>varlist</i><b>)</b> option allows to specify predictors for a specific learner.  If <b>xvars*(</b><i>varlist</i><b>)</b> is missing
    for a specific learner, the default predictor list is used.

<a name="stlog-1-pipelines"></a><b><u>Pipelines</u></b>

    Scikit-learn uses pipelines to pre-preprocess input data on the fly.  Pipelines can be used to impute missing
    observations or create transformation of predictors such as interactions and polynomials.

    The following pipelines are currently supported:

    Pipelines               
      <i>stdscaler</i>             StandardScaler()
      <i>stdscaler0</i>            StandardScaler(with_mean=False)
      <i>sparse</i>                SparseTransformer()
      <i>onehot</i>                OneHotEncoder()()
      <i>minmaxscaler</i>          MinMaxScaler()
      <i>medianimputer</i>         SimpleImputer(strategy='median')
      <i>knnimputer</i>            KNNImputer()
      <i>poly2</i>                 PolynomialFeatures(degree=2)
      <i>poly3</i>                 PolynomialFeatures(degree=3)

    Pipelines can be passed to the base learners via <b>pipe*(</b><i>string</i><b>)</b> (Syntax 1) or <b><u>pipe</u></b><b>line(</b><i>string</i><b>)</b> (Syntax 2).

    <i>stdscaler0</i> is intended for sparse matrices, since <i>stdscaler</i> will make a sparse matrix dense.

<a name="stlog-1-example_boston"></a><b><u>Example using Boston Housing data (Harrison et al., </u></b><a href="#stlog-1-Harrison1978"><b><u>1978</u></b></a><b><u>)</u></b>

<a name="stlog-1-examples_data"></a>    <u>Data set</u>

    The data set is available from the UCI Machine Learning Repository.  The following variables are included in the
    data set of 506 observations:

    Predictors    
      CRIM      per capita crime rate by town
      ZN        proportion of residential land zoned for lots over 25,000 sq.ft.
      INDUS     proportion of non-retail business acres per town
      CHAS      Charles River dummy variable (= 1 if tract bounds river; 0 otherwise)
      NOX       nitric oxides concentration (parts per 10 million)
      RM        average number of rooms per dwelling
      AGE       proportion of owner-occupied units built prior to 1940
      DIS       weighted distances to five Boston employment centres
      RAD       index of accessibility to radial highways
      TAX       full-value property-tax rate per $10,000
      PTRATIO   pupil-teacher ratio by town
      B         1000(Bk - 0.63)^2 where Bk is the proportion Black by town
      LSTAT     % lower status of the population

    Outcome       
      MEDV      Median value of owner-occupied homes in $1000's

    <u>Getting started</u>

    Load housing data.
        . insheet using https://statalasso.github.io/dta/housing.csv, clear

    Stacking regression with lasso, random forest and gradient boosting.
        . pystacked medv crim-lstat, type(regress) pyseed(123) methods(lassocv rf gradboost)

    The weights determine how much each base learner contributes to the final stacking prediction.

    Request the root MSPE table (in-sample only):
        . pystacked, table

    Re-estimate using the first 400 observations, and request the root MSPE table.  RMSPEs for both in-sample (both
    refitted and cross-validated) and the default holdout sample (all unused observations) are reported.:
        . pystacked medv crim-lstat if _n&lt;=400, type(regress) pyseed(123) methods(lassocv rf gradboost)
        . pystacked, table holdout

    Graph predicted vs actual for the holdout sample:
        . pystacked, graph holdout

    Storing the predicted values:
        . predict double yhat, xb

    Storing the cross-validated predicted values:
        . predict double yhat_cv, xb cvalid

    We can also save the predicted values of each base learner:
        . predict double yhat, basexb

    <u>Learner-specific predictors (Syntax 1)</u>

    <b>pystacked</b> allows the use of different sets of predictors for each base learners. For example, linear estimators
    might perform better if interactions are provided as inputs. Here, we use interactions and 2nd-order polynomials
    for the lasso, but not for the other base learners.
        . pystacked medv crim-lstat, type(regress) pyseed(123) methods(ols lassocv rf) xvars2(c.(crim-lstat)#
            #c.(crim-lstat))

    The same can be achieved using pipelines which create polynomials on-the-fly in Python.
        . pystacked medv crim-lstat, type(regress) pyseed(123) methods(ols lassocv rf) pipe2(poly2)

    <u>Learner-specific predictors (Syntax 2)</u>

    We demonstrate the same using the alternative syntax, which is often more handy:

        . pystacked medv crim-lstat || m(ols) || m(lassocv) xvars(c.(crim-lstat)# #c.(crim-lstat)) || m(rf) || ,
            type(regress) pyseed(123)
        . pystacked medv crim-lstat || m(ols) || m(lassocv) pipe(poly2) || m(rf) || , type(regress) pyseed(123)

    <u>Options of base learners (Syntax 1)</u>

    We can pass options to the base learners using <b>cmdopt*(</b><i>string</i><b>)</b>. In this example, we change the maximum tree depth
    for the random forest. Since random forest is the third base learner, we use <b>cmdopt3(</b><i>max_depth</i><b>(</b><i>3</i><b>))</b>.

        . pystacked medv crim-lstat, type(regress) pyseed(123) methods(ols lassocv rf) pipe1(poly2) pipe2(poly2)
            cmdopt3(max_depth(3))

    You can verify that the option has been passed to Python correctly:
        . di e(pyopt3)

    <u>Options of base learners (Syntax 2)</u>

    The same results as above can be achieved using the alternative syntax, which imposes no limit on the number of
    base learners.

        . pystacked medv crim-lstat || m(ols) pipe(poly2) || m(lassocv) pipe(poly2) || m(rf) opt(max_depth(3)) ,
            type(regress) pyseed(123)

    <u>Single base learners</u>

    You can use <b>pystacked</b> with a single base learner.  In this example, we are using a conventional random forest:

        . pystacked medv crim-lstat, type(regress) pyseed(123) methods(rf)

    <u>Voting</u>

    You can also use pre-defined weights. Here, we assign weights of 0.5 to OLS, .1 to the lasso and, implicitly, .4
    to the random foreset.
        . pystacked medv crim-lstat, type(regress) pyseed(123) methods(ols lassocv rf) pipe1(poly2) pipe2(poly2)
            voting voteweights(.5 .1)

<a name="stlog-1-example_spam"></a><b><u>Classification Example using Spam data</u></b>

    <u>Data set</u>

    For demonstration we consider the Spambase Data Set from the UCI Machine Learning Repository.  The data includes
    4,601 observations and 57 variables.  The aim is to predict whether an email is spam (i.e., unsolicited commercial
    e-mail) or not.  Each observation corresponds to one email.

    Predictors    
      v1-v48    percentage of words in the e-mail that match a specific <i>word</i>, i.e. 100 * (number of times the word
                  appears in the e-mail) divided by total number of words in e-mail.  To see which word each predictor
                  corresponds to, see link below.
      v49-v54   percentage of characters in the e-mail that match a specific <i>character</i>, i.e. 100 * (number of times
                  the character appears in the e-mail) divided by total number of characters in e-mail.  To see which
                  character each predictor corresponds to, see link below.
      v55       average length of uninterrupted sequences of capital letters
      v56       length of longest uninterrupted sequence of capital letters
      v57       total number of capital letters in the e-mail

    Outcome       
      v58       denotes whether the e-mail was considered spam (1) or not (0).

    For more information about the data see https://archive.ics.uci.edu/ml/datasets/spambase.

    Load spam data.
        . insheet using https://archive.ics.uci.edu/ml/machine-learning-databases/spambase/spambase.data, clear comma

    Throughout this example, we add the option <b>njobs(</b><i>4</i><b>)</b>, which enables parallelization with 4 cores.

    We consider three base learners: logit, random forest and gradient boosting:
        . pystacked v58 v1-v57, type(class) pyseed(123) methods(logit rf gradboost) njobs(4) pipe1(poly2)

    <u>Out-of-sample classification.</u>

    As the data is ordered by outcome, we first shuffle the data randomly.
        . set seed 42
        . gen u = runiform()
        . sort u

    Estimation on the first 2000 observations.
        . pystacked v58 v1-v57 if _n&lt;=2000, type(class) pyseed(123) methods(logit rf gradboost) njobs(4) pipe1(poly2)

    We can get both the predicted probabilities or the predicted class:
        . predict spam, class
        . predict spam_p, pr

    Confusion matrix, just in-sample and both in- and out-of-sample.
        . pystacked, table
        . pystacked, table holdout

    Confusion matrix for a specified holdout sample.
        . gen h = _n&gt;3000
        . pystacked, table holdout(h)

    ROC curves for the default holdout sample. Specify a subtitle for the combined graph.
        . pystacked, graph(subtitle(Spam data)) holdout

    Predicted probabilites (<b>hist</b> option) for the default holdout sample.  Specify number of bins for the individual
    learner graphs.
        . pystacked, graph hist lgraph(bin(20)) holdout


<a name="stlog-1-installation"></a><b><u>Installation</u></b>

    <b>pystacked</b> requires at least Stata 16 (or higher), a Python installation and scikit-learn (0.24 or higher).  See
    <a href="http://www.stata.com/help.cgi?python"><b>this help file</b></a>, this Stata blog entry and this Youtube video for how to set up Python on your system.  Installing
    Anaconda is in most cases the easiest way of installing Python including all required packages.

    You can check your scikit-learn version using:
        . python: import sklearn
        . python: sklearn.__version__

    <i>Updating scikit-learn:</i>  If you use Anaconda, update scikit-learn through your Anaconda Python distribution. Make
    sure that you have linked Stata with the correct Python installation using python query.

    If you use pip, you can update scikit-learn by typing "&lt;Python path&gt; -m pip install -U scikit-learn" into the
    <i>terminal</i>, or directly in Stata:
        . shell &lt;Python path&gt; -m pip install -U scikit-learn

    Note that you might need to restart Stata for changes to your Python installation to take effect.

    For further information, see https://scikit-learn.org/stable/install.html.

    To install/update <b>pystacked</b>, type
        . net install pystacked, from(https://raw.githubusercontent.com/aahrens1/pystacked/main) replace

<a name="stlog-1-misc"></a><b><u>References</u></b>
<a name="stlog-1-Harrison1978"></a>
    Harrison, D. and Rubinfeld, D.L (1978). Hedonic prices and the demand for clean air. <i>J. Environ. Economics &amp;</i>
    <i>Management</i>, vol.5, 81-102, 1978.

<a name="stlog-1-Hastie2009"></a>    Hastie, T., Tibshirani, R., &amp; Friedman, J. (2009).  The elements of statistical learning: data mining, inference,
    and prediction. Springer Science &amp; Business Media.

<a name="stlog-1-Wolpert1992"></a>    Wolpert, David H. Stacked generalization. <i>Neural networks</i> 5.2 (1992): 241-259.  
    https://doi.org/10.1016/S0893-6080(05)80023-1

<b><u>Contact</u></b>

    If you encounter an error, contact us via email. If you have a question, you can also post on Statalist (please
    tag @Achim Ahrens).

<b><u>Acknowledgements</u></b>

    <b>pystacked</b> took some inspiration from Michael Droste's pylearn, which implements other scikit-learn programs for
    Stata.  Thanks to Jan Ditzen for testing an early version of the program. We also thank Brigham Frandsen and Marco
    Alfano for feedback.  All remaining errors are our own.

<b><u>Citation</u></b>

    Please also cite scikit-learn; see https://scikit-learn.org/stable/about.html.

<b><u>Authors</u></b>

    Achim Ahrens, Public Policy Group, ETH Zurich, Switzerland
    achim.ahrens@gess.ethz.ch

    Christian B. Hansen, University of Chicago, USA
    Christian.Hansen@chicagobooth.edu

    Mark E Schaffer, Heriot-Watt University, UK
</pre>


</article>
 
      

      <footer class="book-footer">
        
  <div class="flex flex-wrap justify-between">





</div>



  <script>(function(){function e(e){const t=window.getSelection(),n=document.createRange();n.selectNodeContents(e),t.removeAllRanges(),t.addRange(n)}document.querySelectorAll("pre code").forEach(t=>{t.addEventListener("click",function(){e(t.parentElement),navigator.clipboard&&navigator.clipboard.writeText(t.parentElement.textContent)})})})()</script>


 
        
      </footer>

      
  
  <div class="book-comments">

</div>
  
 

      <label for="menu-control" class="hidden book-menu-overlay"></label>
    </div>

    
  </main>

  
</body>
</html>












