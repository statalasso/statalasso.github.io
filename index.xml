<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Introduction on Stata ML Page</title>
    <link>https://statalasso.github.io/</link>
    <description>Recent content in Introduction on Stata ML Page</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language><atom:link href="https://statalasso.github.io/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>help lasso2</title>
      <link>https://statalasso.github.io/docs/lassopack/help/lasso2_help/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://statalasso.github.io/docs/lassopack/help/lasso2_help/</guid>
      <description>---------------------------------------------------------------------------------------------------------------------------------- help lasso2 lassopack v1.4.2 ---------------------------------------------------------------------------------------------------------------------------------- Title lasso2 -- Program for lasso, square-root lasso, elastic net, ridge, adaptive lasso and post-estimation OLS Syntax Full syntax lasso2 depvar regressors [if exp] [in range] [, alpha(real) sqrt adaptive adaloadings(string) adatheta(real) ols lambda(numlist) lcount(integer) lminratio(real) lmax(real) lglmnet notpen(varlist) partial(varlist) psolver(string) norecover ploadings(string) unitloadings prestd stdcoef fe noftools noconstant tolopt(real) tolzero(real) maxiter(int) plotpath(method) plotvar(varlist) plotopt(string) plotlabel ic(string) lic(string) ebicgamma(real) noic long displayall postall postresults verbose vverbose wnorm] Note: the fe option will take advantage of the ftools package (if installed) for the fixed-effects transformation; the speed gains using this package can be large.</description>
    </item>
    
    <item>
      <title>Model overview</title>
      <link>https://statalasso.github.io/docs/ddml/models/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://statalasso.github.io/docs/ddml/models/</guid>
      <description>Supported models # Throughout we use \(Y\) to denote the outcome variable, \(X\) to denote confounders, \(Z\) to denote instrumental variable(s), and \(D\) to denote the treatment variable(s) of interest.
For a full discussion, please check our working paper.
Partial linear model [partial] # \[ Y = a.D &amp;#43; g(X) &amp;#43; U \\ D = m(X) &amp;#43; V \quad\quad~~\] where the aim is to estimate \(a\) while flexibly controlling for \(X\) .</description>
    </item>
    
    <item>
      <title>Models</title>
      <link>https://statalasso.github.io/docs/pdslasso/pdslasso_models/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://statalasso.github.io/docs/pdslasso/pdslasso_models/</guid>
      <description>Many instruments # Belloni et al. (2012, Econometrica) consider the model
\[y_i = \alpha d_i &amp;#43; \varepsilon_i \\ d_i = z_i&amp;#39;\delta &amp;#43; u_i\] where \(y_i\) is the dependent variable, \(d_i\) is an endogenous regressors and \(z_i\) is a \(p_z\) -dimensional vector of instruments. \(p_z\) is allowed to be large and may even exceed the sample size. We refer to \(z_i\) as high-dimensional. The interest lies in estimating the causal effect of endogenous variable \(d_i\) on the outcome variable \(y_i\) .</description>
    </item>
    
    <item>
      <title>Package overview</title>
      <link>https://statalasso.github.io/docs/lassopack/package_overview/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://statalasso.github.io/docs/lassopack/package_overview/</guid>
      <description>The package consists of the following programs: # lasso2 implements lasso, square-root lasso, elastic net, ridge regression, adaptive lasso and post-estimation OLS. The lasso (Least Absolute Shrinkage and Selection Operator, Tibshirani 1996), the square-root-lasso (Belloni et al. 2011) and the adaptive lasso (Zou 2006) are regularization methods that use \(\ell_1\) norm penalization to achieve sparse solutions: of the full set of \(p\) predictors, typically most will have coefficients set to zero.</description>
    </item>
    
    <item>
      <title>Algorithm</title>
      <link>https://statalasso.github.io/docs/ddml/crossfit/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://statalasso.github.io/docs/ddml/crossfit/</guid>
      <description>DDML Algorithm # DDML estimators proceed in two stages:
Cross-fitting to estimate conditional expectation functions. Second stage estimation based on Neyman orthogonal scores. Chernozhukov et al. (2018) show that cross-fitting ensures that we can leverage a large class of machine learners for causal inference &amp;ndash; including popular machine learners such as random forests or gradient boosting. Cross-fitting ensures independence between the estimation error from the first step and the regression residual in the second stage.</description>
    </item>
    
    <item>
      <title>Demonstration</title>
      <link>https://statalasso.github.io/docs/pdslasso/pdslasso_demo/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://statalasso.github.io/docs/pdslasso/pdslasso_demo/</guid>
      <description>Demonstration # We demonstrate the use of pdslasso and ivlasso using the data set of Acemoglu, Robinson &amp;amp; Johnson (2001).
. clear . use https://statalasso.github.io/dta/AJR.dta Basic OLS # We are interested in the effect of institutions (measured by avexpr) on income (logpgp95). We ignore endogeneity issues for now and begin with a simple regression of logpgp95 against avexpr:
. reg logpgp95 avexpr Source | SS df MS Number of obs = 64 -------------+---------------------------------- F(1, 62) = 72.</description>
    </item>
    
    <item>
      <title>Estimation methods</title>
      <link>https://statalasso.github.io/docs/lassopack/estimators/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://statalasso.github.io/docs/lassopack/estimators/</guid>
      <description>Ridge regression # The ridge estimator (Hoerl &amp;amp; Kennard, 1970) can be written as
\[\hat{\beta}_{Ridge} = (X&amp;#39;X&amp;#43;\lambda I_p)^{-1}X&amp;#39;y.\] Thus, even if the regressor matrix is not full rank (e.g. because \(p&amp;gt;N\) ), the problem becomes nonsingular by adding a constant to the diagonal of \(X&amp;#39;X\) . Another advantage of the ridge estimator over least squares stems from the variance-bias trade-off. Ridge regression may improve over ordinary least squares by inducing a mild bias while decreasing the variance.</description>
    </item>
    
    <item>
      <title>help cvlasso</title>
      <link>https://statalasso.github.io/docs/lassopack/help/cvlasso_help/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://statalasso.github.io/docs/lassopack/help/cvlasso_help/</guid>
      <description>---------------------------------------------------------------------------------------------------------------------------------- help cvlasso lassopack v1.4.2 ---------------------------------------------------------------------------------------------------------------------------------- Title cvlasso -- Program for cross-validation using lasso, square-root lasso, elastic net, adaptive lasso and post-OLS estimators Syntax Full syntax cvlasso depvar regressors [if exp] [in range] [, alpha(numlist) alphacount(int) sqrt adaptive adaloadings(string) adatheta(real) ols lambda(real) lcount(integer) lminratio(real) lmax(real) lopt lse lglmnet notpen(varlist) partial(varlist) psolver(string) ploadings(string) unitloadings prestd fe noftools noconstant tolopt(real) tolzero(real) maxiter(int) nfolds(int) foldvar(varname) savefoldvar(varname) rolling h(int) origin(int) fixedwindow seed(real) plotcv plotopt(string) saveest(string)] Note: the fe option will take advantage of the ftools package (if installed) for the fixed-effects transform; the speed gains using this package can be large.</description>
    </item>
    
    <item>
      <title>Regularized regression</title>
      <link>https://statalasso.github.io/docs/lassopack/regularized_reg/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://statalasso.github.io/docs/lassopack/regularized_reg/</guid>
      <description>Regularized regression # lasso2 solves the elastic net problem
\[\frac{1}{N} (y_i - x_i&amp;#39;\beta)^2 &amp;#43; \frac{\lambda}{N} \alpha ||\Psi\beta ||_1 &amp;#43; \frac{\lambda}{2N}(1-\alpha)||\Psi\beta||_2\] where
\((y_i - x_i&amp;#39;\beta)^2\) is the residual sum of squares (RSS), \(\beta\) is a \(p\) -dimensional parameter vector, \(\lambda\) is the overall penalty level, which controls the general degree of penalization, \(\alpha\) is the elastic net parameter, which determines the relative contribution of \(\ell_1\) (lasso-type) to \(\ell_2\) (ridge-type) penalization. \(\alpha=1\) corresponds to the lasso; \(\alpha=0\) is ridge regression.</description>
    </item>
    
    <item>
      <title>Getting started</title>
      <link>https://statalasso.github.io/docs/lassopack/lasso2/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://statalasso.github.io/docs/lassopack/lasso2/</guid>
      <description>Load data # For demonstration purposes we use the prostate cancer data set, which has been widely applied to demonstrate the lasso and related techniques.
To load prostate cancer data:
. insheet using /// https://web.stanford.edu/~hastie/ElemStatLearn/datasets/prostate.data, /// clear tab General demonstration # By default, lasso2 uses the lasso estimator (i.e., alpha(1)). Like Stata&amp;rsquo;s regress, lasso2 expects the dependent variable to be named first followed by a list of predictors.
. lasso2 lpsa lcavol lweight age lbph svi lcp gleason pgg45 Knot| ID Lambda s L1-Norm EBIC R-sq | Entered/removed ------+---------------------------------------------------------+---------------- 1| 1 163.</description>
    </item>
    
    <item>
      <title>Getting started</title>
      <link>https://statalasso.github.io/docs/pystacked/getting_started/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://statalasso.github.io/docs/pystacked/getting_started/</guid>
      <description>Getting started # Before we get into stacking, let&amp;rsquo;s first use pystacked as a &amp;ldquo;regular&amp;rdquo; program for machine learning.
Gradient boosting # We load the example data set and randomly split the data in training/test sample.
. clear all . insheet using /// https://statalasso.github.io/dta/housing.csv, /// clear comma . set seed 789 . gen train=runiform() . replace train=train&amp;gt;.75 As an example, we run pystacked with gradient boosting:
. pystacked medv crim-lstat if train, /// type(regress) method(gradboost) Single base learner: no stacking done.</description>
    </item>
    
    <item>
      <title>help  rlasso</title>
      <link>https://statalasso.github.io/docs/lassopack/help/rlasso_help/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://statalasso.github.io/docs/lassopack/help/rlasso_help/</guid>
      <description>---------------------------------------------------------------------------------------------------------------------------------- help rlasso lassopack v1.4.2 ---------------------------------------------------------------------------------------------------------------------------------- Title rlasso -- Program for lasso and sqrt-lasso estimation with data-driven penalization Syntax rlasso depvar regressors [weight] [if exp] [in range] [ , sqrt partial(varlist) pnotpen(varlist) psolver(string) norecover noconstant fe noftools robust cluster(varlist) bw(int) kernel(string) center xdependent numsim(int) prestd tolopt(real) tolpsi(real) tolzero(real) maxiter(int) maxpsiiter(int) maxabsx lassopsi corrnumber(int) lalternative gamma(real) maq c(real) c0(real) supscore ssnumsim(int) testonly seed(real) displayall postall ols verbose vverbose ] Note: the fe option will take advantage of the ftools package (if installed) for the fixed-effects transform; the speed gains using this package can be large.</description>
    </item>
    
    <item>
      <title>Partial Linear Model (PLM)</title>
      <link>https://statalasso.github.io/docs/ddml/plm/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://statalasso.github.io/docs/ddml/plm/</guid>
      <description>Partially Linear Model # Preparations # We load the data, define global macros and set the seed.
. use https://github.com/aahrens1/ddml/raw/master/data/sipp1991.dta, clear . global Y net_tfa . global D e401 . global X tw age inc fsize educ db marr twoearn pira hown . set seed 42 Step 1: Initialize DDML model # We next initialize the ddml estimation and select the model. partial refers to the partially linear model. The model will be stored on a Mata object with the default name &amp;ldquo;m0&amp;rdquo; unless otherwise specified using the mname(name) option.</description>
    </item>
    
    <item>
      <title>help lassologit</title>
      <link>https://statalasso.github.io/docs/lassopack/help/lassologit_help/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://statalasso.github.io/docs/lassopack/help/lassologit_help/</guid>
      <description></description>
    </item>
    
    <item>
      <title>PLM &amp; Stacking</title>
      <link>https://statalasso.github.io/docs/ddml/plm2/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://statalasso.github.io/docs/ddml/plm2/</guid>
      <description>Partially linear model with Stacking # Stacking regression is a simple and powerful method for combining predictions from multiple learners. It is available in Stata via the pystacked package (see here). Below is an example with the partially linear model, but it can be used with any model supported by ddml.
Step 1: Initialization # Preparation: use the data and globals as above. Use the name m1 for this new estimation, to distinguish it from the previous example that uses the default name m0.</description>
    </item>
    
    <item>
      <title>Interactive</title>
      <link>https://statalasso.github.io/docs/ddml/interactive/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://statalasso.github.io/docs/ddml/interactive/</guid>
      <description>Interactive Model # Preparations: we load the data, define global macros and set the seed.
. webuse cattaneo2, clear (Excerpt from Cattaneo (2010) Journal of Econometrics 155: 138â€“154) . global Y bweight . global D mbsmoke . global X mage prenatal1 mmarried fbaby mage medu . set seed 42 Step 1: Initialization # We use 5 folds and 5 resamplings; that is, we estimate the model 5 times using randomly chosen folds.</description>
    </item>
    
    <item>
      <title>Panel FE</title>
      <link>https://statalasso.github.io/docs/pdslasso/pdslasso_panel/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://statalasso.github.io/docs/pdslasso/pdslasso_panel/</guid>
      <description>Panel FE and Clustering # pdslasso and ivlasso can also be applied to fixed effect panel models using the methodology of Belloni et al., 2014. Since the appropriate level of regularization depends on the error structure, we need to accommodate cluster dependence that is likely to be present in panel data. Ignoring cluster dependence would lead to a regularization level that is too low.
For demonstation, we consider the nlswork example data set:</description>
    </item>
    
    <item>
      <title>Partial Linear IV</title>
      <link>https://statalasso.github.io/docs/ddml/iv/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://statalasso.github.io/docs/ddml/iv/</guid>
      <description>Partial Linear IV Model # Preparations # We load the data, define global macros and set the seed.
. use https://statalasso.github.io/dta/AJR.dta, clear . global Y logpgp95 . global D avexpr . global Z logem4 . global X lat_abst edes1975 avelf temp* humid* steplow-oilres . set seed 42 Step 1: Initialization # Since the data set is very small, we consider 30 cross-fitting folds.
. ddml init iv, kfolds(30) Step 2: Adding learners # The partially linear IV model has three conditional expectations: \(E[Y|X]\) , \(E[D|X]\) and \(E[Z|X]\) .</description>
    </item>
    
    <item>
      <title>Flexible IV</title>
      <link>https://statalasso.github.io/docs/ddml/ivhd/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://statalasso.github.io/docs/ddml/ivhd/</guid>
      <description>Flexible Partially Linear IV Model # Preparations # We load the data, define global macros and set the seed.
. use https://statalasso.github.io/dta/BLP_CHS.dta, clear . global Y y . global D price . global X hpwt air mpd space . global Z Zbase* . set seed 42 Step 1: Initialization # We initialize the model.
. ddml init ivhd Step 2: Add learners # We add learners for \(E[Y|X]\) in the usual way.</description>
    </item>
    
    <item>
      <title>Cross-validation</title>
      <link>https://statalasso.github.io/docs/lassopack/cvlasso/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://statalasso.github.io/docs/lassopack/cvlasso/</guid>
      <description>Cross-validation # In the course of cross-validation, the data is repeatedly partitioned into training and validation data. The model is fit to the training data and the validation data is used to calculate the prediction error. This in turn enables us to identify the values of \(\lambda\) and \(\alpha\) that optimize predictive performance (i.e., minimize the estimated mean-squared prediction error).
cvlasso supports \(K\) -fold cross-validation and \(h\) -step ahead rolling cross-validation.</description>
    </item>
    
    <item>
      <title>Interactive IV</title>
      <link>https://statalasso.github.io/docs/ddml/interactiveiv/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://statalasso.github.io/docs/ddml/interactiveiv/</guid>
      <description>Interactive IV Model # Preparations # We load the data, define global macros and set the seed.
. use http://fmwww.bc.edu/repec/bocode/j/jtpa.dta,clear . global Y earnings . global D training . global Z assignmt . global X sex age married black hispanic . set seed 42 Step 1: Initialization # We initialize the model.
. ddml init interactiveiv, kfolds(5) Step 2: Add learners # We use stacking (implemented in pystacked) with two base learners for each reduced form equation.</description>
    </item>
    
    <item>
      <title>Regression</title>
      <link>https://statalasso.github.io/docs/pystacked/regression/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://statalasso.github.io/docs/pystacked/regression/</guid>
      <description>Stacking regression # First load the Boston housing data and split the data randomly in training and test sample:
. insheet using /// https://statalasso.github.io/dta/housing.csv, /// clear comma . set seed 789 . gen train=runiform() . replace train=train&amp;gt;.75 We now consider a more complicated pystacked application with 5 base learners: linear regression, two versions of lasso with AIC-chosen penalty, random forest and gradient boosting:
. pystacked medv crim-lstat if train, /// type(regress) /// methods(ols lassoic lassoic rf gradboost) /// pipe1(poly2) pipe3(poly2) cmdopt5(learning_rate(0.</description>
    </item>
    
    <item>
      <title>Rigorous lasso</title>
      <link>https://statalasso.github.io/docs/lassopack/rlasso/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://statalasso.github.io/docs/lassopack/rlasso/</guid>
      <description>Theory driven penalty # rlasso provides routines for estimating the coefficients of a lasso or square-root lasso regression with data-dependent, theory-driven penalization. The number of regressors, \(p\) , may be large and possibly greater than the number of observations, \(N\) . rlasso implements a version of the lasso that allows for heteroskedastic and clustered errors; see Belloni et al. (2012, 2016).
We start again with the prostate cancer data for demonstration.</description>
    </item>
    
    <item>
      <title>Classification</title>
      <link>https://statalasso.github.io/docs/pystacked/classification/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://statalasso.github.io/docs/pystacked/classification/</guid>
      <description>Stacking classifier # Stacking can be applied in a similar way to classification problems. For demonstration, we consider the Spambase Data Set from the Machine Learning Repository. We load the data and shuffle the observations around since they are ordered by outcome.
. insheet using /// https://archive.ics.uci.edu/ml/machine-learning-databases/spambase/spambase.data, /// clear comma . set seed 42 . gen uni=runiform() . sort uni Stacking classification works very similar to stacking regression. The example below is somewhat more complicated.</description>
    </item>
    
    <item>
      <title>Help file</title>
      <link>https://statalasso.github.io/docs/pdslasso/ivlasso_help/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://statalasso.github.io/docs/pdslasso/ivlasso_help/</guid>
      <description>---------------------------------------------------------------------------------------------------------------------------------- help pdslasso, help ivlasso pdslasso v1.3 ---------------------------------------------------------------------------------------------------------------------------------- Title pdslasso and ivlasso -- Programs for post-selection and post-regularization OLS or IV estimation and inference Syntax pdslasso depvar regressors (hd_controls) [weight] [if exp] [in range] [ , partial(varlist) pnotpen(varlist) psolver(string) aset(varlist) post(method) robust cluster(varlist) bw(int) kernel(string) fe noftools rlasso[(name)] sqrt noisily loptions(options) olsoptions(options) noconstant ] ivlasso depvar regressors [(hd_controls)] (endog=instruments) [if exp] [in range] [ , partial(varlist) pnotpen(varlist) psolver(string) aset(varlist) post(method) robust cluster(varlist) bw(int) kernel(string) fe noftools rlasso[(name)] sqrt noisily loptions(options) ivoptions(options) first idstats sscset ssgamma(real) ssgridmin(real) ssgridmax(real) ssgridpoints(integer 100) ssgridmat(name) noconstant ] Note: pdslasso requires rlasso and ivreg2 to be installed; ivlasso also requires ranktest.</description>
    </item>
    
    <item>
      <title>Parallelization</title>
      <link>https://statalasso.github.io/docs/pystacked/parallel/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://statalasso.github.io/docs/pystacked/parallel/</guid>
      <description>Parallelization # pystacked can be run in parallel, even without a StataMP license.
pystacked can be parallelized at the level of the base learners or at the stacking level (to speed up the cross-validation process). Example 1 below uses no parallelization (the default). Example 2 parallelizes the random forest base learner. Example 3 parallelizes at the top level.
. insheet using /// &amp;gt; https://archive.ics.uci.edu/ml/machine-learning-databases/spambase/spambase.data, /// &amp;gt; clear comma . set seed 42 .</description>
    </item>
    
    <item>
      <title>ddml help file</title>
      <link>https://statalasso.github.io/docs/ddml/help/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://statalasso.github.io/docs/ddml/help/</guid>
      <description>----------------------------------------------------------------------------------------------------------------------------------------------------------------- help ddml v1.2 ----------------------------------------------------------------------------------------------------------------------------------------------------------------- Title ddml -- Stata package for Double Debiased Machine Learning ddml implements algorithms for causal inference aided by supervised machine learning as proposed in Double/debiased machine learning for treatment and structural parameters (Econometrics Journal, 2018). Five different models are supported, allowing for binary or continous treatment variables and endogeneity, high-dimensional controls and/or instrumental variables. ddml supports a variety of different ML programs, including but not limited to lassopack and pystacked.</description>
    </item>
    
    <item>
      <title>Help file</title>
      <link>https://statalasso.github.io/docs/pystacked/help/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://statalasso.github.io/docs/pystacked/help/</guid>
      <description>------------------------------------------------------------------------------------------------------------------------ help pystacked v0.4.8 ------------------------------------------------------------------------------------------------------------------------ Title pystacked -- Stata program for Stacking Regression Overview pystacked implements stacking regression (Wolpert, 1992) via scikit-learn&#39;s sklearn.ensemble.StackingRegressor and sklearn.ensemble.StackingClassifier. Stacking is a way of combining multiple supervised machine learners (the &#34;base&#34; or &#34;level-0&#34; learners) into a meta learner. The currently supported base learners are linear regression, logit, lasso, ridge, elastic net, (linear) support vector machines, gradient boosting, and neural nets (MLP). pystacked can also be used with a single base learner and, thus, provides an easy-to-use API for scikit-learn&#39;s machine learning algorithms.</description>
    </item>
    
    <item>
      <title>qddml help file</title>
      <link>https://statalasso.github.io/docs/ddml/help_qddml/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://statalasso.github.io/docs/ddml/help_qddml/</guid>
      <description>----------------------------------------------------------------------------------------------------------------------------------------------------------------- help ddml v1.2 ----------------------------------------------------------------------------------------------------------------------------------------------------------------- Title qddml -- Stata program for Double Debiased Machine Learning ddml implements algorithms for causal inference aided by supervised machine learning as proposed in Double/debiased machine learning for treatment and structural parameters (Econometrics Journal, 2018). Five different models are supported, allowing for binary or continous treatment variables and endogeneity, high-dimensional controls and/or instrumental variables. ddml supports a variety of different ML programs, including but not limited to lassopack and pystacked.</description>
    </item>
    
    <item>
      <title>Comparison glmnet</title>
      <link>https://statalasso.github.io/docs/lassopack/lasso2_replication/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://statalasso.github.io/docs/lassopack/lasso2_replication/</guid>
      <description>Replication of glmnet and StataCorp&amp;rsquo;s lasso # Use Stata&amp;rsquo;s auto dataset with missing data dropped. The variable price1000 is used to illustrate scaling effects.
. sysuse auto, clear . drop if rep78==. . gen double price1000 = price/1000 Replication of glmnet # To load the data into R for comparison with glmnet, use the following commands. The packages haven and tidyr need to be installed.
&amp;gt; auto &amp;lt;- haven::read_dta(&amp;quot;http://www.stata-press.com/data/r9/auto.dta&amp;quot;) &amp;gt; auto &amp;lt;- tidyr::drop_na() &amp;gt; n &amp;lt;- nrow(auto) &amp;gt; price &amp;lt;- auto$price &amp;gt; X &amp;lt;- auto[, c(&amp;quot;mpg&amp;quot;, &amp;quot;rep78&amp;quot;, &amp;quot;headroom&amp;quot;, &amp;quot;trunk&amp;quot;, &amp;quot;weight&amp;quot;, &amp;quot;length&amp;quot;, &amp;quot;turn&amp;quot;, &amp;quot;displacement&amp;quot;, &amp;quot;gear_ratio&amp;quot;, &amp;quot;foreign&amp;quot;)] &amp;gt; X$foreign &amp;lt;- as.</description>
    </item>
    
    <item>
      <title>Installation</title>
      <link>https://statalasso.github.io/docs/ddml/installation/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://statalasso.github.io/docs/ddml/installation/</guid>
      <description>Installation # You can get the lastest versions from github:
net install ddml, from(https://raw.githubusercontent.com/aahrens1/ddml/master) To install an older version, use for example:
net install ddml, from(https://raw.githubusercontent.com/aahrens1/ddml/v1.2) Please check for updates on a regular basis.
Offline installation # If you want to use ddml in an offline environment, we recommend to download the packages from the Github repositories. The links to repositories are above; click the green button &amp;ldquo;Code&amp;rdquo; and &amp;ldquo;Download ZIP&amp;rdquo;.</description>
    </item>
    
    <item>
      <title>Installation</title>
      <link>https://statalasso.github.io/docs/pdslasso/installation/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://statalasso.github.io/docs/pdslasso/installation/</guid>
      <description>SSC version # You can get pdslasso from SSC:
ssc install pdslasso Note that pdslasso requires lassopack to be installed.
Add replace to overwrite existing versions of the packages.
Github installation # Please note that we update the SSC versions less frequently. You can get the lastest versions from github:
net install pdslasso, /// from(&amp;quot;https://raw.githubusercontent.com/statalasso/pdslasso/master/&amp;quot;) Please check for updates on a regular basis.
Installing old versions: # We keep old versions of lassopack and pdslasso on github to facilitate reproducibility.</description>
    </item>
    
    <item>
      <title>Citation</title>
      <link>https://statalasso.github.io/docs/pdslasso/pdslasso_cite/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://statalasso.github.io/docs/pdslasso/pdslasso_cite/</guid>
      <description>Citation # pdslasso and ivlasso are not official Stata commands. They are free contributions to the research community, like a paper.
Please cite it as such:
Ahrens, A., Hansen, C.B., Schaffer, M.E. 2018. pdslasso and ivlasso: Programs for post-selection and post-regularization OLS or IV estimation and inference. http://ideas.repec.org/c/boc/bocode/s458459.html
Bibtex file</description>
    </item>
    
    <item>
      <title>Installation</title>
      <link>https://statalasso.github.io/docs/lassopack/installation/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://statalasso.github.io/docs/lassopack/installation/</guid>
      <description>SSC version # You can get lassopack from SSC:
ssc install lassopack Add replace to overwrite existing versions of the packages.
Github installation # Please note that we update the SSC versions less frequently. You can get the lastest versions from github:
net install pdslasso, /// from(&amp;quot;https://raw.githubusercontent.com/statalasso/pdslasso/master/&amp;quot;) Please check for updates on a regular basis.
Installing old versions: # We keep old versions of lassopack on github to facilitate reproducibility.</description>
    </item>
    
    <item>
      <title>Installation</title>
      <link>https://statalasso.github.io/docs/pystacked/installation/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://statalasso.github.io/docs/pystacked/installation/</guid>
      <description>Installation # You can get the lastest versions from Github:
net install pystacked, /// from(https://raw.githubusercontent.com/aahrens1/pystacked/main) replace Please check for updates on a regular basis.
pystacked requires at least Stata 16 (or higher), a Python installation and scikit-learn (0.24 or higher). Python and scikit-learn are available for free. You can also install from SSC, but note that we update the SSC version less regularly:
ssc install pystacked Install old versions # To install an old version:</description>
    </item>
    
    <item>
      <title>Citation</title>
      <link>https://statalasso.github.io/docs/lassopack/lassopack_cite/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://statalasso.github.io/docs/lassopack/lassopack_cite/</guid>
      <description>Citation # lassopack is not an official Stata package. It is a free contribution to the research community, like a paper.
Please cite it as such:
Ahrens, A., Hansen, C.B., Schaffer, M.E. 2018. LASSOPACK: Stata module for lasso, square-root lasso, elastic net, ridge, adaptive lasso estimation and cross-validation. http://ideas.repec.org/c/boc/bocode/s458458.html
Bibtex file
Ahrens A, Hansen CB, Schaffer ME (2020). lassopack: Model selection and prediction with regularized regression in Stata. The Stata Journal.</description>
    </item>
    
    <item>
      <title>Citation</title>
      <link>https://statalasso.github.io/docs/pystacked/citation/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://statalasso.github.io/docs/pystacked/citation/</guid>
      <description>Citation # pystacked is not an official Stata command. It&amp;rsquo;s a free contribution to the research community, like a paper. Please cite it as such.
To cite our working paper:
@misc{https://doi.org/10.48550/arxiv.2208.10896, doi = {10.48550/ARXIV.2208.10896}, url = {https://arxiv.org/abs/2208.10896}, author = {Ahrens, Achim and Hansen, Christian B. and Schaffer, Mark E.}, keywords = {Econometrics (econ.EM), Machine Learning (stat.ML), FOS: Economics and business, FOS: Economics and business, FOS: Computer and information sciences, FOS: Computer and information sciences}, title = {pystacked: Stacking generalization and machine learning in Stata}, publisher = {arXiv}, year = {2022}, copyright = {Creative Commons Attribution Share Alike 4.</description>
    </item>
    
    <item>
      <title>Example using Spam data</title>
      <link>https://statalasso.github.io/docs/lassopack/lassologit/lassologit_demo/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://statalasso.github.io/docs/lassopack/lassologit/lassologit_demo/</guid>
      <description>Logistic Lasso: Spam data # For demonstration we consider the Spambase Data Set from the Machine Learning Repository. The data set includes 4,601 observations and 57 variables. The aim is to predict if an email is spam (i.e., unsolicited commercial e-mail) or not. Each observation corresponds to one email.
Predictors v1-v48 percentage of words in the e-mail that match a specific word, i.e. 100 * (number of times the word appears in the e-mail) divided by total number of words in e-mail.</description>
    </item>
    
  </channel>
</rss>
